{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13DP2Og8rRB3"
      },
      "source": [
        "# Taxi Gridworld Agent with OpenAI Gymnasium (Taxi-v3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4Eov6tPrPRU"
      },
      "source": [
        "This project trains and evaluates a simple reinforcement-learning-ready agent in the classic Taxi gridworld environment using OpenAI Gymnasium’s Taxi-v3 task. It provides a minimal agent–environment interaction loop, performance monitoring, and a clean starting point for implementing Q-learning, SARSA, or other value-based methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2tPfeJjrY5P"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This project implements a minimal RL agent interacting with the classic **Taxi** environment, originally from OpenAI Gym and now available via **Gymnasium** as `Taxi-v3`. The agent maintains a state–action table `Q[s][a]` (initialized via `defaultdict`) and runs for many episodes to collect rewards, making it a clean starting point for experimenting with **Q-learning, SARSA, and other tabular RL algorithms** in a discrete gridworld.\n",
        "\n",
        "The environment is a **5×5 grid** with walls, four landmark locations (**R, G, Y, B**), and a single taxi that must:\n",
        "1. Navigate to the passenger,\n",
        "2. Pick them up,\n",
        "3. Navigate to the destination,\n",
        "4. Drop them off.\n",
        "\n",
        "The key details (as defined in the Taxi environment and Dietterich’s MAXQ paper) are:\n",
        "\n",
        "- **Action space (6 discrete actions)**  \n",
        "  `0` = South, `1` = North, `2` = East, `3` = West, `4` = Pickup, `5` = Dropoff  \n",
        "\n",
        "- **State space (500 discrete states)**  \n",
        "  Encodes: taxi row (5) × taxi column (5) × passenger location (5: R, G, Y, B, in-taxi) × destination (4: R, G, Y, B) → **500 states total**.\n",
        "\n",
        "- **Rewards**  \n",
        "  - `-1` per time step (including bumps into walls) → encourages shortest routes  \n",
        "  - `+20` for successful drop-off at correct destination (episode ends)  \n",
        "  - `-10` for illegal pickup/dropoff actions  \n",
        "\n",
        "The provided `interact` loop tracks **average reward over a sliding window** and prints the **best average reward**, stopping once the task is considered solved (e.g., average reward ≥ 9.7 over the last 100 episodes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "USztwzlqrlRi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym  # modern replacement for gym\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Tabular Q-learning agent with novelty-biased exploration.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Q-table: maps state -> np.array of action values (size nA)\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Path memory: counts how often each action is taken in each state recently.\n",
        "        # Used to bias exploration toward less-frequently-taken actions.\n",
        "        self.path_counts = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # --- Hyperparameters (tuned for Taxi-v3) ---\n",
        "        self.alpha = 0.1          # initial learning rate\n",
        "        self.alpha_min = 0.01     # minimum learning rate\n",
        "        self.alpha_decay = 0.9995 # decay per episode\n",
        "\n",
        "        self.gamma = 0.99         # discount factor\n",
        "\n",
        "        # ε-greedy with novelty-biased softmax for exploration\n",
        "        self.epsilon = 1.0        # initial exploration rate\n",
        "        self.epsilon_min = 0.05   # minimum exploration rate\n",
        "        self.epsilon_decay = 0.9995  # decay per episode\n",
        "\n",
        "        # Softmax exploration parameters when we *do* explore\n",
        "        self.beta_q = 3.0         # weight for Q-values in softmax\n",
        "        self.beta_novelty = 1.0   # weight for novelty (path_counts) in softmax\n",
        "        self.path_decay = 0.9     # geometric decay of path memory per episode\n",
        "\n",
        "        self.episode = 0          # count completed episodes\n",
        "\n",
        "    def _softmax_explore(self, state):\n",
        "        \"\"\"Sample an action using softmax over (Q - novelty).\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        counts = self.path_counts[state]\n",
        "\n",
        "        # Higher Q is good, higher counts mean \"already tried a lot\" → subtract them.\n",
        "        prefs = self.beta_q * q_vals - self.beta_novelty * counts\n",
        "\n",
        "        # Numerical stability\n",
        "        prefs = prefs - np.max(prefs)\n",
        "        exp_prefs = np.exp(prefs)\n",
        "        probs = exp_prefs / np.sum(exp_prefs)\n",
        "\n",
        "        return np.random.choice(self.nA, p=probs)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Given the state, select an action using ε-greedy with novelty-biased exploration.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - state: the current state of the environment\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "        - action: an integer, compatible with the task's action space\n",
        "        \"\"\"\n",
        "        # Exploit (greedy) with probability 1 - ε\n",
        "        if np.random.random() > self.epsilon:\n",
        "            return np.argmax(self.Q[state])\n",
        "\n",
        "        # Explore with probability ε: softmax over (Q - novelty)\n",
        "        return self._softmax_explore(state)\n",
        "\n",
        "    def _end_of_episode_update(self):\n",
        "        \"\"\"Updates done once per episode: decay ε, α, and path memory.\"\"\"\n",
        "        self.episode += 1\n",
        "\n",
        "        # Decay epsilon (exploration rate), but keep it above epsilon_min\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "        # Decay learning rate slightly toward alpha_min\n",
        "        self.alpha = max(self.alpha_min, self.alpha * self.alpha_decay)\n",
        "\n",
        "        # Geometric decay for path memory: keeps it focused on *recent* behavior\n",
        "        for s in self.path_counts:\n",
        "            self.path_counts[s] *= self.path_decay\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Update the agent's knowledge using the Q-learning update rule.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - state: the previous state of the environment\n",
        "        - action: the agent's previous choice of action\n",
        "        - reward: last reward received\n",
        "        - next_state: the current state of the environment\n",
        "        - done: whether the episode is complete (True or False)\n",
        "        \"\"\"\n",
        "        # Standard Q-learning target\n",
        "        best_next_action_value = 0.0 if done else np.max(self.Q[next_state])\n",
        "        td_target = reward + self.gamma * best_next_action_value\n",
        "        td_error = td_target - self.Q[state][action]\n",
        "\n",
        "        # Q-learning update\n",
        "        self.Q[state][action] += self.alpha * td_error\n",
        "\n",
        "        # Update path memory (for novelty-based exploration)\n",
        "        self.path_counts[state][action] += 1.0\n",
        "\n",
        "        # If episode finished, perform per-episode updates (ε, α, path memory)\n",
        "        if done:\n",
        "            self._end_of_episode_update()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQWp_CyBropB"
      },
      "outputs": [],
      "source": [
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance.\n",
        "\n",
        "    Params\n",
        "    ======\n",
        "    - env: instance of Gymnasium's Taxi-v3 environment\n",
        "    - agent: instance of class Agent\n",
        "    - num_episodes: number of episodes of agent-environment interaction\n",
        "    - window: number of episodes to consider when calculating average rewards\n",
        "\n",
        "    Returns\n",
        "    =======\n",
        "    - avg_rewards: deque containing average rewards\n",
        "    - best_avg_reward: largest value in the avg_rewards deque\n",
        "    \"\"\"\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best average reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.3f} α={agent.alpha:.3f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # \"Solved\" threshold (as in Udacity / Gym examples)\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "        if i_episode == num_episodes:\n",
        "            print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ_uqeeyrsBy",
        "outputId": "5fd0cda3-0f39-4993-eccf-538e81b0f8c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best average reward 8.71 || ε=0.050 α=0.010\n",
            "\n",
            "\n",
            "\n",
            "Final best average reward: 8.71\n"
          ]
        }
      ],
      "source": [
        "# ==== main notebook entry point ====\n",
        "env = gym.make(\"Taxi-v3\")  # Gymnasium Taxi-v3\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(\"\\n\\nFinal best average reward:\", best_avg_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjVfdg4zuUsE"
      },
      "source": [
        "Experiment 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7kdpv1EvBZ0",
        "outputId": "aa05d718-bde2-4ccf-94d1-db1b9d487dd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best average reward 8.37\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym  # updated: use gymnasium instead of gym\n",
        "\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Initialize agent.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Hyperparameters for Q-learning\n",
        "        self.alpha = 0.1  # learning rate\n",
        "        self.gamma = 0.99  # discount factor\n",
        "\n",
        "        # Epsilon-greedy parameters\n",
        "        self.epsilon = 1.0  # initial exploration rate\n",
        "        self.epsilon_min = 0.01  # minimum exploration rate\n",
        "        self.epsilon_decay = 0.9999  # decay rate per episode\n",
        "\n",
        "        self.episode_count = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Given the state, select an action using epsilon-greedy policy.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - state: the current state of the environment\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "        - action: an integer, compatible with the task's action space\n",
        "        \"\"\"\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: choose random action\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            # Exploit: choose best action based on current Q-values\n",
        "            return np.argmax(self.Q[state])\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Update the agent's knowledge using Q-learning update rule.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - state: the previous state of the environment\n",
        "        - action: the agent's previous choice of action\n",
        "        - reward: last reward received\n",
        "        - next_state: the current state of the environment\n",
        "        - done: whether the episode is complete (True or False)\n",
        "        \"\"\"\n",
        "        # Q-learning update rule:\n",
        "        # Q(s,a) ← Q(s,a) + α[r + γ·max Q(s',a') - Q(s,a)]\n",
        "\n",
        "        if done:\n",
        "            # Terminal state: no future rewards\n",
        "            target = reward\n",
        "        else:\n",
        "            # Use max Q-value of next state for update\n",
        "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        # Update Q-value\n",
        "        self.Q[state][action] += self.alpha * (target - self.Q[state][action])\n",
        "\n",
        "        # Decay epsilon after each step\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance.\n",
        "\n",
        "    Params\n",
        "    ======\n",
        "    - env: instance of Gymnasium's Taxi-v3 environment\n",
        "    - agent: instance of class Agent\n",
        "    - num_episodes: number of episodes of agent-environment interaction\n",
        "    - window: number of episodes to consider when calculating average rewards\n",
        "\n",
        "    Returns\n",
        "    =======\n",
        "    - avg_rewards: deque containing average rewards\n",
        "    - best_avg_reward: largest value in the avg_rewards deque\n",
        "    \"\"\"\n",
        "    # initialize average rewards\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    # initialize best average reward\n",
        "    best_avg_reward = -math.inf\n",
        "    # initialize monitor for most recent rewards\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    # for each episode\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        # Gymnasium: reset returns (obs, info)\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            # agent selects an action\n",
        "            action = agent.select_action(state)\n",
        "            # Gymnasium: step returns (obs, reward, terminated, truncated, info)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # agent performs internal updates based on sampled experience\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            # update the sampled reward\n",
        "            samp_reward += reward\n",
        "            # update the state\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            # get average reward from last window episodes\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        # monitor progress\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || Best average reward {best_avg_reward:.2f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # check if task is solved (Gym definition used in many examples)\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== main notebook entry point ====\n",
        "# create environment and agent, then run training\n",
        "env = gym.make(\"Taxi-v3\")  # updated: Taxi-v3 with Gymnasium\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Y3Rl0j1mzK"
      },
      "source": [
        "Improvement of the above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXtTJpFW1oqr",
        "outputId": "895f5857-be21-4e2f-936e-90e52863c2d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best average reward 8.62 || ε=0.001\n",
            "\n",
            "\n",
            "Final best average reward: 8.62\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym  # updated: use gymnasium instead of gym\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Tabular Q-learning agent for Taxi-v3.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Q-table: maps state -> array of action-values (size nA)\n",
        "        # Zero init (this matched your best grid-search config)\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Q-learning hyperparameters\n",
        "        self.alpha = 0.1      # learning rate (fixed)\n",
        "        self.gamma = 0.99     # discount factor\n",
        "\n",
        "        # Epsilon-greedy exploration hyperparameters\n",
        "        self.epsilon = 1.0        # initial exploration rate\n",
        "        self.epsilon_min = 0.001  # minimum exploration rate (fast-decay config)\n",
        "        self.epsilon_decay = 0.995  # decay per EPISODE (not per step!)\n",
        "\n",
        "        self.episode_count = 0    # counts completed episodes\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Return a greedy action for a given state, breaking ties randomly.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        # All actions with the max Q-value\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        # Break ties randomly among equally good actions\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action using epsilon-greedy policy.\n",
        "\n",
        "        With probability epsilon: choose a random action (explore).\n",
        "        With probability 1 - epsilon: choose greedy action wrt Q (exploit).\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: uniform over all actions\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            # Exploit: greedy action (with random tie-breaking)\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def _end_of_episode_update(self):\n",
        "        \"\"\"Decay epsilon once per episode.\"\"\"\n",
        "        self.episode_count += 1\n",
        "        # Per-episode epsilon decay schedule (fast decay)\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update using the most recent transition.\n",
        "\n",
        "        Q(s,a) ← Q(s,a) + α [ r + γ max_a' Q(s',a') - Q(s,a) ]\n",
        "        \"\"\"\n",
        "        # Current Q(s,a)\n",
        "        q_sa = self.Q[state][action]\n",
        "\n",
        "        # Target for Q-learning\n",
        "        if done:\n",
        "            # No future value if this is a terminal transition\n",
        "            target = reward\n",
        "        else:\n",
        "            # Max over next state's action-values\n",
        "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        # TD error\n",
        "        td_error = target - q_sa\n",
        "\n",
        "        # Q-learning update\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # If the episode has finished, do per-episode housekeeping (epsilon decay)\n",
        "        if done:\n",
        "            self._end_of_episode_update()\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance.\n",
        "\n",
        "    Params\n",
        "    ======\n",
        "    - env: instance of Gymnasium's Taxi-v3 environment\n",
        "    - agent: instance of class Agent\n",
        "    - num_episodes: number of episodes of agent-environment interaction\n",
        "    - window: number of episodes to consider when calculating average rewards\n",
        "\n",
        "    Returns\n",
        "    =======\n",
        "    - avg_rewards: deque containing average rewards\n",
        "    - best_avg_reward: largest value in the avg_rewards deque\n",
        "    \"\"\"\n",
        "    # initialize average rewards\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    # initialize best average reward\n",
        "    best_avg_reward = -math.inf\n",
        "    # initialize monitor for most recent rewards\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    # for each episode\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        # Gymnasium: reset returns (obs, info)\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            # agent selects an action\n",
        "            action = agent.select_action(state)\n",
        "            # Gymnasium: step returns (obs, reward, terminated, truncated, info)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # agent performs internal updates based on sampled experience\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            # update the sampled reward\n",
        "            samp_reward += reward\n",
        "            # update the state\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            # get average reward from last window episodes\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        # monitor progress\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best average reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.3f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # check if task is solved (Gym definition used in many examples)\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== main notebook entry point ====\n",
        "env = gym.make(\"Taxi-v3\")  # updated: Taxi-v3 with Gymnasium\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(\"\\nFinal best average reward:\", best_avg_reward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxdra9XbbSWx"
      },
      "source": [
        "## Novelty-Softmax Q-Learning\n",
        "\n",
        "Standard Q-learning + ε-greedy, but when exploring, we sample actions from a softmax that prefers (1) higher-Q actions and (2) less-used actions in that state, tracked via a decaying path-memory table. This is inspired by Andy Harless’ leaderboard solution and is designed to squeeze out those last +0.5–1.0 average reward points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIuuAflQbTr-",
        "outputId": "f44cfeb8-20f7-45b5-988e-ff804cdc46e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best average reward 9.01 || ε=0.001 α=0.050\n",
            "\n",
            "\n",
            "Final best average reward: 9.01\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym  # updated: use gymnasium instead of gym\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Novelty-Softmax Q-learning agent for Taxi-v3.\n",
        "\n",
        "        - Base algorithm: tabular Q-learning\n",
        "        - Exploration: epsilon-greedy at top level\n",
        "          * Greedy: argmax_a Q(s,a)\n",
        "          * Exploratory: softmax over (beta_q * Q(s,a) - beta_n * path_counts(s,a))\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Q-table: maps state -> array of action-values (size nA)\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Path memory: how often each action was taken in each state (recently).\n",
        "        # This is used as a \"novelty penalty\" in exploration.\n",
        "        self.path_counts = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Q-learning hyperparameters\n",
        "        self.alpha = 0.1         # initial learning rate\n",
        "        self.alpha_min = 0.05    # don't go below this (small decay for stability)\n",
        "        self.alpha_decay = 0.999 # per-episode decay (very gentle)\n",
        "        self.gamma = 0.99        # discount factor\n",
        "\n",
        "        # Epsilon-greedy exploration hyperparameters (per EPISODE)\n",
        "        self.epsilon = 1.0         # initial exploration rate\n",
        "        self.epsilon_min = 0.001   # minimum exploration rate\n",
        "        self.epsilon_decay = 0.995 # fast decay (your best config)\n",
        "\n",
        "        # Softmax exploration hyperparameters\n",
        "        self.beta_q = 3.0          # weight for Q-values in softmax\n",
        "        self.beta_novelty = 1.0    # weight for novelty penalty (path_counts)\n",
        "        self.path_decay = 0.9      # geometric decay of path_counts per episode\n",
        "\n",
        "        self.episode_count = 0\n",
        "\n",
        "    # ---------- Action selection helpers ----------\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Return greedy action for a given state, breaking ties randomly.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def _softmax_explore_action(self, state):\n",
        "        \"\"\"Sample an exploratory action via softmax over (Q - novelty).\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        counts = self.path_counts[state]\n",
        "\n",
        "        # Preferences: high Q is good; high count (over-used) is bad.\n",
        "        prefs = self.beta_q * q_vals - self.beta_novelty * counts\n",
        "\n",
        "        # Numerical stability\n",
        "        prefs = prefs - np.max(prefs)\n",
        "        exp_prefs = np.exp(prefs)\n",
        "        probs = exp_prefs / np.sum(exp_prefs)\n",
        "\n",
        "        return np.random.choice(self.nA, p=probs)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action using epsilon-greedy with novelty-softmax exploration.\n",
        "\n",
        "        With probability epsilon: explore (softmax over Q - novelty).\n",
        "        With probability 1 - epsilon: exploit (greedy wrt Q).\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return self._softmax_explore_action(state)\n",
        "        else:\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    # ---------- Episode-level housekeeping ----------\n",
        "\n",
        "    def _end_of_episode_update(self):\n",
        "        \"\"\"Decay epsilon, alpha, and path memory once per episode.\"\"\"\n",
        "        self.episode_count += 1\n",
        "\n",
        "        # Per-episode epsilon decay\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "        # Gentle alpha decay (keeps early learning fast, late learning stable)\n",
        "        self.alpha = max(self.alpha_min, self.alpha * self.alpha_decay)\n",
        "\n",
        "        # Geometric decay of path_counts to emphasize RECENT behavior\n",
        "        for s in self.path_counts:\n",
        "            self.path_counts[s] *= self.path_decay\n",
        "\n",
        "    # ---------- Learning update ----------\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update using the most recent transition.\n",
        "\n",
        "        Q(s,a) ← Q(s,a) + α [ r + γ max_a' Q(s',a') - Q(s,a) ]\n",
        "        \"\"\"\n",
        "        # Current Q(s,a)\n",
        "        q_sa = self.Q[state][action]\n",
        "\n",
        "        # Target for Q-learning\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        # TD error and update\n",
        "        td_error = target - q_sa\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # Update path memory (for novelty-aware exploration)\n",
        "        self.path_counts[state][action] += 1.0\n",
        "\n",
        "        # If episode finished, do per-episode housekeeping\n",
        "        if done:\n",
        "            self._end_of_episode_update()\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance.\n",
        "\n",
        "    Params\n",
        "    ======\n",
        "    - env: instance of Gymnasium's Taxi-v3 environment\n",
        "    - agent: instance of class Agent\n",
        "    - num_episodes: number of episodes of agent-environment interaction\n",
        "    - window: number of episodes to consider when calculating average rewards\n",
        "\n",
        "    Returns\n",
        "    =======\n",
        "    - avg_rewards: deque containing average rewards\n",
        "    - best_avg_reward: largest value in the avg_rewards deque\n",
        "    \"\"\"\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best average reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.3f} α={agent.alpha:.3f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== main notebook entry point ====\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(\"\\nFinal best average reward:\", best_avg_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUvlEITZbVXH"
      },
      "source": [
        "## Gemini: Optimized Epsilon-Decay Q-Learning\n",
        "\n",
        "This revised agent uses a slower, step-based decay for the $\\epsilon$ (exploration) parameter, allowing for a more sustained exploration period. It also adjusts the epsilon_decay and alpha hyperparameters based on common successful configurations for the Taxi-v3 environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJiSZw7-b9QC",
        "outputId": "a76836c3-fb5b-4db2-dd4d-f72a3bc7583b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best average reward 9.14 || ε=0.00100\n",
            "\n",
            "\n",
            "Final best average reward: 9.14\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "# === Optimized Epsilon-Decay Q-Learning ===\n",
        "# Strategy: Slower, step-based epsilon decay and slightly tuned hyperparameters\n",
        "# to ensure more thorough exploration of the state space.\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Tabular Q-learning agent for Taxi-v3 with optimized hyperparameters.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Q-table: maps state -> array of action-values (size nA)\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Q-learning hyperparameters\n",
        "        self.alpha = 0.08     # Learning rate: Slightly reduced from 0.1\n",
        "        self.gamma = 0.99     # Discount factor (kept high)\n",
        "\n",
        "        # Epsilon-greedy exploration hyperparameters (CRITICAL CHANGES)\n",
        "        self.epsilon = 1.0        # Initial exploration rate\n",
        "        self.epsilon_min = 0.001  # Minimum exploration rate\n",
        "        # Slower, step-based decay for prolonged exploration: 0.99995 is common\n",
        "        self.epsilon_decay = 0.99995\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Return a greedy action for a given state, breaking ties randomly.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        # All actions with the max Q-value\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        # Break ties randomly among equally good actions\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action using epsilon-greedy policy.\n",
        "\n",
        "        This method now also handles the step-based epsilon decay.\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: uniform over all actions\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            # Exploit: greedy action (with random tie-breaking)\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def _update_epsilon(self):\n",
        "        \"\"\"Decay epsilon once per step (not per episode).\"\"\"\n",
        "        # Step-based epsilon decay schedule\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update and step-based epsilon decay.\"\"\"\n",
        "        # Current Q(s,a)\n",
        "        q_sa = self.Q[state][action]\n",
        "\n",
        "        # Target for Q-learning\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            # Max over next state's action-values\n",
        "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        # TD error\n",
        "        td_error = target - q_sa\n",
        "\n",
        "        # Q-learning update\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # CRITICAL CHANGE: Decay epsilon *per step*\n",
        "        self._update_epsilon()\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance (No change needed here, it's robust).\"\"\"\n",
        "    # initialize average rewards\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    # initialize best average reward\n",
        "    best_avg_reward = -math.inf\n",
        "    # initialize monitor for most recent rewards\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    # for each episode\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        # Gymnasium: reset returns (obs, info)\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            # agent selects an action\n",
        "            action = agent.select_action(state)\n",
        "            # Gymnasium: step returns (obs, reward, terminated, truncated, info)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # agent performs internal updates based on sampled experience\n",
        "            # Epsilon decay is now *inside* agent.step\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            # update the sampled reward\n",
        "            samp_reward += reward\n",
        "            # update the state\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            # get average reward from last window episodes\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        # monitor progress\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best average reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.5f}\", # Showing more digits for epsilon decay\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # check if task is solved (Gym definition used in many examples)\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== main notebook entry point ====\n",
        "env = gym.make(\"Taxi-v3\", render_mode=None)\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(\"\\nFinal best average reward:\", best_avg_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni_oRYgJ08DK"
      },
      "source": [
        "## Variation of the above with more episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8a7Vztpyz7Qu",
        "outputId": "6771af5d-1298-4425-e76d-7ba3dc0256aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 150000/150000 || Best average reward 8.93 || ε=0.00055\n",
            "\n",
            "\n",
            "Final best average reward: 8.93\n"
          ]
        }
      ],
      "source": [
        "# Trying the same as above but with more episodes and a few more changes\n",
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "# === Q-Learning with Andy-like Hyperparams & Epsilon Schedule ===\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Tabular Q-learning agent for Taxi-v3 with tuned hyperparameters.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Q-table: maps state -> array of action-values (size nA)\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Q-learning hyperparameters (aligned with Andy's setup)\n",
        "        self.alpha = 0.7      # learning rate (aggressive, as in Andy's code)\n",
        "        self.gamma = 0.5      # discount factor (shorter-sighted, as in Andy)\n",
        "\n",
        "        # Epsilon schedule parameters (per EPISODE, like Andy)\n",
        "        self.a = -0.005\n",
        "        self.b = 5e-5\n",
        "        self.epsilon_min = 0.0\n",
        "\n",
        "        # Track episodes to drive epsilon schedule\n",
        "        self.episode_idx = 0\n",
        "\n",
        "        # Initial epsilon (episode 0)\n",
        "        self.epsilon = max(self.epsilon_min, math.exp(self.a - self.b * self.episode_idx))\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Return a greedy action for a given state, breaking ties randomly.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: uniform over all actions\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            # Exploit: greedy action (with random tie-breaking)\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def _update_epsilon_episode(self):\n",
        "        \"\"\"Update epsilon once per episode using Andy-style schedule.\"\"\"\n",
        "        self.episode_idx += 1\n",
        "        self.epsilon = max(self.epsilon_min,\n",
        "                           math.exp(self.a - self.b * self.episode_idx))\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update; epsilon decay happens per EPISODE.\"\"\"\n",
        "        # Current Q(s,a)\n",
        "        q_sa = self.Q[state][action]\n",
        "\n",
        "        # Target for Q-learning\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        # TD error\n",
        "        td_error = target - q_sa\n",
        "\n",
        "        # Q-learning update\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # If the episode ended, update epsilon for the *next* episode\n",
        "        if done:\n",
        "            self._update_epsilon_episode()\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=150000, window=100):\n",
        "    \"\"\"Monitor agent's performance.\"\"\"\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        # Gymnasium: reset returns (obs, info)\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            # agent selects an action\n",
        "            action = agent.select_action(state)\n",
        "            # Gymnasium: step returns (obs, reward, terminated, truncated, info)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # agent performs internal updates based on sampled experience\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            # update the sampled reward\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            # get average reward from last window episodes\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        # monitor progress\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best average reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.5f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== main notebook entry point ====\n",
        "env = gym.make(\"Taxi-v3\")  # Gymnasium env id\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(\"\\nFinal best average reward:\", best_avg_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F154zYop1cij",
        "outputId": "4c89054e-5b7b-428c-b9de-2d2c9ebf68ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 150000/150000 || Best average reward 9.05 || ε=0.00100\n",
            "\n",
            "\n",
            "Final best average reward: 9.05\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "# === Optimized Epsilon-Decay Q-Learning ===\n",
        "# Strategy: Slower, step-based epsilon decay and slightly tuned hyperparameters\n",
        "# to ensure more thorough exploration of the state space.\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Tabular Q-learning agent for Taxi-v3 with optimized hyperparameters.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Q-table: maps state -> array of action-values (size nA)\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Q-learning hyperparameters\n",
        "        self.alpha = 0.08     # Learning rate: Slightly reduced from 0.1\n",
        "        self.gamma = 0.99     # Discount factor (kept high)\n",
        "\n",
        "        # Epsilon-greedy exploration hyperparameters (CRITICAL CHANGES)\n",
        "        self.epsilon = 1.0        # Initial exploration rate\n",
        "        self.epsilon_min = 0.001  # Minimum exploration rate\n",
        "        # Slower, step-based decay for prolonged exploration: 0.99995 is common\n",
        "        self.epsilon_decay = 0.99995\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Return a greedy action for a given state, breaking ties randomly.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        # All actions with the max Q-value\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        # Break ties randomly among equally good actions\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action using epsilon-greedy policy.\n",
        "\n",
        "        This method now also handles the step-based epsilon decay.\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: uniform over all actions\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            # Exploit: greedy action (with random tie-breaking)\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def _update_epsilon(self):\n",
        "        \"\"\"Decay epsilon once per step (not per episode).\"\"\"\n",
        "        # Step-based epsilon decay schedule\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update and step-based epsilon decay.\"\"\"\n",
        "        # Current Q(s,a)\n",
        "        q_sa = self.Q[state][action]\n",
        "\n",
        "        # Target for Q-learning\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            # Max over next state's action-values\n",
        "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        # TD error\n",
        "        td_error = target - q_sa\n",
        "\n",
        "        # Q-learning update\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # CRITICAL CHANGE: Decay epsilon *per step*\n",
        "        self._update_epsilon()\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=150000, window=100):\n",
        "    \"\"\"Monitor agent's performance (No change needed here, it's robust).\"\"\"\n",
        "    # initialize average rewards\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    # initialize best average reward\n",
        "    best_avg_reward = -math.inf\n",
        "    # initialize monitor for most recent rewards\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    # for each episode\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        # Gymnasium: reset returns (obs, info)\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            # agent selects an action\n",
        "            action = agent.select_action(state)\n",
        "            # Gymnasium: step returns (obs, reward, terminated, truncated, info)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # agent performs internal updates based on sampled experience\n",
        "            # Epsilon decay is now *inside* agent.step\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            # update the sampled reward\n",
        "            samp_reward += reward\n",
        "            # update the state\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            # get average reward from last window episodes\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        # monitor progress\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best average reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.5f}\",  # Showing more digits for epsilon decay\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # check if task is solved (Gym definition used in many examples)\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== main notebook entry point ====\n",
        "env = gym.make(\"Taxi-v3\")  # <- fixed env id\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(\"\\nFinal best average reward:\", best_avg_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn8ULLUr2_o0",
        "outputId": "a1e2e372-fd60-49b2-9483-a2133e26a447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 150000/150000 || Best average reward 9.19 || ε=0.00100\n",
            "\n",
            "\n",
            "Final best average reward: 9.19\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "from gymnasium.envs.toy_text.taxi import TaxiEnv\n",
        "\n",
        "# Register an alias \"Taxi-v3a\" that uses the same TaxiEnv\n",
        "gym.register(\n",
        "    id=\"Taxi-v3a\",\n",
        "    entry_point=\"gymnasium.envs.toy_text.taxi:TaxiEnv\",\n",
        ")\n",
        "\n",
        "env = gym.make(\"Taxi-v3a\")\n",
        "\n",
        "\n",
        "# === Optimized Epsilon-Decay Q-Learning ===\n",
        "# Strategy: Slower, step-based epsilon decay and slightly tuned hyperparameters\n",
        "# to ensure more thorough exploration of the state space.\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Tabular Q-learning agent for Taxi-v3 with optimized hyperparameters.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Q-table: maps state -> array of action-values (size nA)\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Q-learning hyperparameters\n",
        "        self.alpha = 0.08     # Learning rate: Slightly reduced from 0.1\n",
        "        self.gamma = 0.99     # Discount factor (kept high)\n",
        "\n",
        "        # Epsilon-greedy exploration hyperparameters (CRITICAL CHANGES)\n",
        "        self.epsilon = 1.0        # Initial exploration rate\n",
        "        self.epsilon_min = 0.001  # Minimum exploration rate\n",
        "        # Slower, step-based decay for prolonged exploration: 0.99995 is common\n",
        "        self.epsilon_decay = 0.99995\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Return a greedy action for a given state, breaking ties randomly.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        # All actions with the max Q-value\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        # Break ties randomly among equally good actions\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action using epsilon-greedy policy.\n",
        "\n",
        "        This method now also handles the step-based epsilon decay.\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: uniform over all actions\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            # Exploit: greedy action (with random tie-breaking)\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def _update_epsilon(self):\n",
        "        \"\"\"Decay epsilon once per step (not per episode).\"\"\"\n",
        "        # Step-based epsilon decay schedule\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update and step-based epsilon decay.\"\"\"\n",
        "        # Current Q(s,a)\n",
        "        q_sa = self.Q[state][action]\n",
        "\n",
        "        # Target for Q-learning\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            # Max over next state's action-values\n",
        "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        # TD error\n",
        "        td_error = target - q_sa\n",
        "\n",
        "        # Q-learning update\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # CRITICAL CHANGE: Decay epsilon *per step*\n",
        "        self._update_epsilon()\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=150000, window=100):\n",
        "    \"\"\"Monitor agent's performance (No change needed here, it's robust).\"\"\"\n",
        "    # initialize average rewards\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    # initialize best average reward\n",
        "    best_avg_reward = -math.inf\n",
        "    # initialize monitor for most recent rewards\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    # for each episode\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        # Gymnasium: reset returns (obs, info)\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            # agent selects an action\n",
        "            action = agent.select_action(state)\n",
        "            # Gymnasium: step returns (obs, reward, terminated, truncated, info)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # agent performs internal updates based on sampled experience\n",
        "            # Epsilon decay is now *inside* agent.step\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            # update the sampled reward\n",
        "            samp_reward += reward\n",
        "            # update the state\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            # get average reward from last window episodes\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        # monitor progress\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best average reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.5f}\", # Showing more digits for epsilon decay\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # check if task is solved (Gym definition used in many examples)\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== main notebook entry point ====\n",
        "env = gym.make(\"Taxi-v3a\")\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(\"\\nFinal best average reward:\", best_avg_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDbi6Y5hkUg2"
      },
      "source": [
        "## Softmax novelty q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-4722k1UkUNu",
        "outputId": "d5252c78-3821-4c9a-c7a5-1518c9709c25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best average reward 8.74 || ε=0.00100\n",
            "\n",
            "\n",
            "Final best average reward: 8.74\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "# === Softmax Novelty Q-Learning Agent =======================================\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Q-learning agent for Taxi-v3 with novelty-aware softmax exploration.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Q-table: state -> action-values\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Path memory: counts how often we've taken each action in each state\n",
        "        # (recently, because we decay it each episode)\n",
        "        self.path = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # --- Core Q-learning hyperparameters (kept conservative) -------------\n",
        "        self.alpha = 0.08       # learning rate (slightly smaller than 0.1)\n",
        "        self.gamma = 0.99       # discount factor\n",
        "\n",
        "        # --- Epsilon-greedy exploration (step-based decay) -------------------\n",
        "        self.epsilon = 1.0          # start fully exploring\n",
        "        self.epsilon_min = 0.001    # floor\n",
        "        self.epsilon_decay = 0.99995  # decay per step (slow-ish, your best so far)\n",
        "\n",
        "        # --- Softmax exploration over (Q - novelty) --------------------------\n",
        "        # Higher beta_Q = more strongly follow Q-values when exploring\n",
        "        # Higher beta_P = stronger penalty for over-used actions (path memory)\n",
        "        self.beta_Q = 1.0\n",
        "        self.beta_P = 1.0\n",
        "\n",
        "        # Path memory decay per episode (0<path_decay<1)\n",
        "        #  -> 0.9 = keep strong memory of current episode, weak of very old ones\n",
        "        self.path_decay = 0.9\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Utility functions\n",
        "    # -------------------------------------------------------------------------\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Return a greedy action for a given state, breaking ties randomly.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def _softmax_explore_action(self, state):\n",
        "        \"\"\"Sample an action using softmax over (beta_Q * Q - beta_P * path).\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        p_vals = self.path[state]\n",
        "\n",
        "        # Preferences combine \"greedy\" (Q) and \"novelty\" (negative path count)\n",
        "        prefs = self.beta_Q * q_vals - self.beta_P * p_vals\n",
        "\n",
        "        # Numerical stability: subtract max before exp\n",
        "        prefs = prefs - np.max(prefs)\n",
        "        exp_prefs = np.exp(prefs)\n",
        "        probs = exp_prefs / np.sum(exp_prefs)\n",
        "\n",
        "        # Sample according to the softmax probabilities\n",
        "        return np.random.choice(self.nA, p=probs)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Public API used by monitor.py / interact()\n",
        "    # -------------------------------------------------------------------------\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action using epsilon-greedy with softmax exploration.\"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # EXPLORE: not uniform, but softmax over Q and path memory\n",
        "            action = self._softmax_explore_action(state)\n",
        "        else:\n",
        "            # EXPLOIT: standard greedy wrt Q(s,·)\n",
        "            action = self._greedy_action(state)\n",
        "        return action\n",
        "\n",
        "    def _decay_epsilon_step(self):\n",
        "        \"\"\"Decay epsilon once per step.\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def _decay_path_episode(self):\n",
        "        \"\"\"Decay the entire path-memory table at the end of each episode.\"\"\"\n",
        "        # path is small (≤500 reachable states * 6 actions), so iterating is fine.\n",
        "        for s in list(self.path.keys()):\n",
        "            self.path[s] *= self.path_decay\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update and exploration bookkeeping.\n",
        "\n",
        "        Q(s,a) ← Q(s,a) + α [ r + γ max_a' Q(s',a') − Q(s,a) ]\n",
        "        \"\"\"\n",
        "        # 1) Update path memory: we used (state, action) once more\n",
        "        self.path[state][action] += 1.0\n",
        "\n",
        "        # 2) Standard tabular Q-learning\n",
        "        q_sa = self.Q[state][action]\n",
        "\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        td_error = target - q_sa\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # 3) Decay epsilon *per step* (your best-performing schedule)\n",
        "        self._decay_epsilon_step()\n",
        "\n",
        "        # 4) Episode-level bookkeeping\n",
        "        if done:\n",
        "            # decay novelty counts so recent actions matter more than ancient ones\n",
        "            self._decay_path_episode()\n",
        "\n",
        "\n",
        "# === interact() unchanged ====================================================\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance.\"\"\"\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best average reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.5f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== main notebook entry point ==============================================\n",
        "env = gym.make(\"Taxi-v3\")  # <-- correct environment ID\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(\"\\nFinal best average reward:\", best_avg_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyH1Sj0Zlb2r"
      },
      "source": [
        "## bump alpha from 0.08 to 0.12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OeCKJL_mlbpz",
        "outputId": "ae221dbb-65c8-450a-b894-daf0813ed2a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best average reward 8.76 || ε=0.00100\n",
            "\n",
            "\n",
            "Final best average reward: 8.76\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "# === Q-Learning with Step-Based Epsilon Decay (α = 0.12) ===\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Tabular Q-learning agent for Taxi-v3 with tuned hyperparameters.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Q-table: maps state -> array of action-values (size nA)\n",
        "        # Zero initialization (proved most stable in your experiments)\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Q-learning hyperparameters\n",
        "        self.alpha = 0.12    # ⬆️ slightly increased from 0.08\n",
        "        self.gamma = 0.99    # high discount factor\n",
        "\n",
        "        # Epsilon-greedy exploration (step-based decay)\n",
        "        self.epsilon = 1.0          # initial exploration\n",
        "        self.epsilon_min = 0.001    # floor\n",
        "        self.epsilon_decay = 0.99995  # per-STEP decay (your best schedule)\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Return a greedy action for a given state, breaking ties randomly.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: uniform random over actions\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            # Exploit: greedy (with random tie-breaking)\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def _update_epsilon(self):\n",
        "        \"\"\"Decay epsilon once per step.\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update and step-based epsilon decay.\"\"\"\n",
        "        # Current Q(s,a)\n",
        "        q_sa = self.Q[state][action]\n",
        "\n",
        "        # Target for Q-learning\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        # TD error and update\n",
        "        td_error = target - q_sa\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # Decay epsilon every step (fine-grained exploration schedule)\n",
        "        self._update_epsilon()\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance.\"\"\"\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best average reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.5f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== main notebook entry point ====\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(\"\\nFinal best average reward:\", best_avg_reward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DV9C8nEfywE"
      },
      "source": [
        "# A few experiments (variations of above)\n",
        "\n",
        "## Verison A: Optimistic Init + Two-Phase Alpha (Most Promising!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xTTeJUoif6of",
        "outputId": "73d9dce4-452c-4c14-ea9b-9b4884e45647"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best avg reward 8.84 || ε=0.00100 α=0.050\n",
            "\n",
            "\n",
            "Final best average reward: 8.84\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Q-learning with optimistic init and adaptive learning rate.\n",
        "\n",
        "        Key improvements:\n",
        "        - Optimistic initialization (Q=1.5) for exploration\n",
        "        - Two-phase alpha: fast learning then refinement\n",
        "        - Step-based epsilon decay (proven to work)\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # CHANGE 1: Optimistic initialization\n",
        "        self.Q = defaultdict(lambda: np.ones(self.nA) * 1.5)\n",
        "\n",
        "        # CHANGE 2: Two-phase learning rate\n",
        "        self.alpha_high = 0.15   # Early learning\n",
        "        self.alpha_low = 0.05    # Late refinement\n",
        "        self.alpha_transition = 10000  # Switch point\n",
        "        self.step_count = 0\n",
        "\n",
        "        self.gamma = 0.99\n",
        "\n",
        "        # Keep your proven epsilon schedule\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.001\n",
        "        self.epsilon_decay = 0.99995\n",
        "\n",
        "    @property\n",
        "    def alpha(self):\n",
        "        \"\"\"Adaptive learning rate based on step count.\"\"\"\n",
        "        if self.step_count < self.alpha_transition:\n",
        "            return self.alpha_high\n",
        "        else:\n",
        "            # Smooth transition\n",
        "            progress = (self.step_count - self.alpha_transition) / 200000\n",
        "            return max(self.alpha_low, self.alpha_high - progress * (self.alpha_high - self.alpha_low))\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Return greedy action with random tie-breaking.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update with adaptive alpha.\"\"\"\n",
        "        self.step_count += 1\n",
        "\n",
        "        q_sa = self.Q[state][action]\n",
        "        target = reward if done else reward + self.gamma * np.max(self.Q[next_state])\n",
        "        td_error = target - q_sa\n",
        "\n",
        "        # Use adaptive alpha\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # Decay epsilon per step\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance.\"\"\"\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best avg reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.5f} α={agent.alpha:.3f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        if best_avg_reward >= 9.27:\n",
        "            print(f\"\\n🎯 TARGET REACHED in {i_episode} episodes!\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== Run Variant A ====\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(f\"\\nFinal best average reward: {best_avg_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6yfUUEvf73w"
      },
      "source": [
        "## Version B: Lower Gamma (More Myopic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5wv78uWKf9MS",
        "outputId": "f2af2392-5025-45d2-8537-be1ce919cd47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best avg reward 8.67 || ε=0.00100\n",
            "\n",
            "\n",
            "Final best average reward: 8.67\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Q-learning with lower gamma for myopic planning.\n",
        "\n",
        "        Key change: gamma=0.95 makes agent prioritize immediate rewards more.\n",
        "        This can help find shorter paths in grid worlds.\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "        self.Q = defaultdict(lambda: np.ones(self.nA) * 1.0)  # Mild optimism\n",
        "\n",
        "        self.alpha = 0.1\n",
        "        self.gamma = 0.95  # CHANGE: Lower discount factor\n",
        "\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.001\n",
        "        self.epsilon_decay = 0.99995\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        q_sa = self.Q[state][action]\n",
        "        target = reward if done else reward + self.gamma * np.max(self.Q[next_state])\n",
        "        td_error = target - q_sa\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best avg reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.5f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        if best_avg_reward >= 9.27:\n",
        "            print(f\"\\n🎯 TARGET REACHED in {i_episode} episodes!\")\n",
        "            break\n",
        "\n",
        "    print(\"\\n\")\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(f\"\\nFinal best average reward: {best_avg_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M17QZotigM-C"
      },
      "source": [
        "## Variant C: Three-Phase Epsilon (Exploration → Exploitation → Refinement)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wYtys1eFgUq1",
        "outputId": "bd26674a-cac0-4048-b611-ec1f49747cf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best avg reward 6.68 || ε=0.05054\n",
            "\n",
            "\n",
            "Final best average reward: 6.68\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Q-learning with three-phase epsilon decay.\n",
        "\n",
        "        Phase 1 (0-300k steps): Fast exploration (ε: 1.0 → 0.1)\n",
        "        Phase 2 (300k-600k steps): Moderate exploitation (ε: 0.1 → 0.01)\n",
        "        Phase 3 (600k+ steps): Refinement (ε: 0.01 → 0.001)\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "        self.Q = defaultdict(lambda: np.ones(self.nA) * 1.2)\n",
        "\n",
        "        self.alpha = 0.1\n",
        "        self.gamma = 0.99\n",
        "\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.001\n",
        "        self.step_count = 0\n",
        "\n",
        "        # Three-phase thresholds\n",
        "        self.phase1_steps = 300000\n",
        "        self.phase2_steps = 600000\n",
        "\n",
        "    def _get_epsilon_decay(self):\n",
        "        \"\"\"Adaptive epsilon decay based on training phase.\"\"\"\n",
        "        if self.step_count < self.phase1_steps:\n",
        "            return 0.999992  # Fast decay: 1.0 → ~0.1 in 300k steps\n",
        "        elif self.step_count < self.phase2_steps:\n",
        "            return 0.999995  # Medium decay: 0.1 → ~0.01 in next 300k\n",
        "        else:\n",
        "            return 0.999998  # Slow decay: 0.01 → 0.001\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.step_count += 1\n",
        "\n",
        "        q_sa = self.Q[state][action]\n",
        "        target = reward if done else reward + self.gamma * np.max(self.Q[next_state])\n",
        "        td_error = target - q_sa\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # Adaptive epsilon decay\n",
        "        decay_rate = self._get_epsilon_decay()\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * decay_rate)\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best avg reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.5f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        if best_avg_reward >= 9.27:\n",
        "            print(f\"\\n🎯 TARGET REACHED in {i_episode} episodes!\")\n",
        "            break\n",
        "\n",
        "    print(\"\\n\")\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(f\"\\nFinal best average reward: {best_avg_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ss98z_-3c4TV"
      },
      "source": [
        "# Optimistic Q-Learning with Decaying Alpha\n",
        "\n",
        "This strategy employs two main mechanisms to overcome local optima and ensure convergence:\n",
        "\n",
        "1. Optimistic Initialization: The Q-table is initialized with a high value (e.g., 5.0). Since most rewards are less than 5, the agent is incentivized to explore every state-action pair aggressively to 'correct' the overestimation, ensuring comprehensive coverage of the state space.\n",
        "\n",
        "2. Decaying Learning Rate ($\\alpha$): The learning rate starts high (e.g., 0.5) for aggressive initial learning and then decays slowly per episode down to a minimum (e.g., 0.01). This allows for quick, large updates early on and guarantees finer, stable convergence later in the training process.\n",
        "\n",
        "The $\\epsilon$-decay remains step-based, as this proved effective in the previous iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z6XvKHTYdD1j",
        "outputId": "5e79a6d5-7cfb-4807-d8b8-1fd04a2d00eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best avg reward 8.67 || α=0.0100 | ε=0.00100\n",
            "\n",
            "\n",
            "Final best average reward: 8.67\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "# === Optimistic Q-Learning with Decaying Alpha (Dual Decay) ===\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Tabular Q-learning agent using Optimistic Initialization and Dual Decay.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Q-table: Initialize optimistically to 5.0.\n",
        "        # This encourages initial exploration, as the agent seeks to disprove\n",
        "        # the overly high reward estimates.\n",
        "        self.Q = defaultdict(lambda: np.full(self.nA, 5.0))\n",
        "\n",
        "        # Q-learning hyperparameters\n",
        "        self.alpha = 0.5      # Learning rate: Start high for fast initial updates\n",
        "        self.alpha_min = 0.01 # Minimum learning rate\n",
        "        self.alpha_decay = 0.9995 # Decay factor per EPISODE\n",
        "        self.gamma = 0.99     # Discount factor (kept high)\n",
        "\n",
        "        # Epsilon-greedy exploration hyperparameters (Per-Step Decay)\n",
        "        self.epsilon = 1.0        # Initial exploration rate\n",
        "        self.epsilon_min = 0.001  # Minimum exploration rate\n",
        "        self.epsilon_decay = 0.99995  # Decay per STEP (slower overall)\n",
        "\n",
        "        self.episode_count = 0    # Counts completed episodes\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Return a greedy action for a given state, breaking ties randomly.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: uniform over all actions\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            # Exploit: greedy action\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def _update_epsilon(self):\n",
        "        \"\"\"Decay epsilon once per step.\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def _update_alpha(self):\n",
        "        \"\"\"Decay alpha once per episode.\"\"\"\n",
        "        self.episode_count += 1\n",
        "        self.alpha = max(self.alpha_min, self.alpha * self.alpha_decay)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update with dual decay.\"\"\"\n",
        "        # Current Q(s,a)\n",
        "        q_sa = self.Q[state][action]\n",
        "\n",
        "        # Target calculation\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            # Max over next state's action-values (Q-learning)\n",
        "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        # TD error\n",
        "        td_error = target - q_sa\n",
        "\n",
        "        # Q-learning update using current alpha\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # Update exploration rate (per step)\n",
        "        self._update_epsilon()\n",
        "\n",
        "        # Update learning rate (per episode)\n",
        "        if done:\n",
        "            self._update_alpha()\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance.\"\"\"\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Agent performs Q-update and decay\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        # Monitor progress (now includes current alpha)\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best avg reward {best_avg_reward:.2f} || \"\n",
        "            f\"α={agent.alpha:.4f} | ε={agent.epsilon:.5f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # check if task is solved\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== main notebook entry point ====\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(\"\\nFinal best average reward:\", best_avg_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21zUlxY2eQkE"
      },
      "source": [
        "## Conservative Stable Q-Learning (Slower $\\epsilon$ Decay)\n",
        "\n",
        "We are leveraging the successful configuration (step-based $\\epsilon$-decay, non-optimistic initialization) and making two minor, stabilizing adjustments: slightly reducing the learning rate ($\\alpha$) to ensure convergence is smooth, and slightly slowing the $\\epsilon$-decay to prolong the final, necessary exploration of hard-to-reach states.Change 1 (Stability): Reduce $\\alpha$ from $0.08$ to $\\mathbf{0.06}$ for more conservative updates.Change 2 (Exploration): Slow $\\epsilon$-decay from $0.99995$ to $\\mathbf{0.99998}$ to keep the agent searching for the optimal path for a longer fraction of the 20,000 episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2rnYXXpneZcs",
        "outputId": "13efb9a9-361f-4775-ea27-2816eee7096e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best avg reward 8.68 || ε=0.00100\n",
            "\n",
            "\n",
            "Final best average reward: 8.68\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "# === SARSA (State-Action-Reward-State-Action) Control ===\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Tabular SARSA agent for Taxi-v3.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Q-table: maps state -> array of action-values (size nA)\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # SARSA hyperparameters (reverted to successful Q-learning baseline)\n",
        "        self.alpha = 0.08     # Learning rate\n",
        "        self.gamma = 0.99     # Discount factor\n",
        "\n",
        "        # Epsilon-greedy exploration hyperparameters (Per-Step Decay)\n",
        "        self.epsilon = 1.0        # Initial exploration rate\n",
        "        self.epsilon_min = 0.001  # Minimum exploration rate\n",
        "        self.epsilon_decay = 0.99995 # Slower decay per STEP\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Return a greedy action for a given state, breaking ties randomly.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: uniform over all actions\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            # Exploit: greedy action\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def _update_epsilon(self):\n",
        "        \"\"\"Decay epsilon once per step.\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, next_action, done):\n",
        "        \"\"\"SARSA update and step-based epsilon decay.\n",
        "\n",
        "        SARSA update uses the Q-value of the *next action (next_action)*.\n",
        "        Q(s,a) ← Q(s,a) + α [ r + γ Q(s',a') - Q(s,a) ]\n",
        "        \"\"\"\n",
        "        # Current Q(s,a)\n",
        "        q_sa = self.Q[state][action]\n",
        "\n",
        "        # Target calculation (SARSA: uses Q[next_state][next_action])\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            # CRITICAL CHANGE: Use Q-value of the actual action taken in next_state\n",
        "            target = reward + self.gamma * self.Q[next_state][next_action]\n",
        "\n",
        "        # TD error\n",
        "        td_error = target - q_sa\n",
        "\n",
        "        # SARSA update\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # Update exploration rate (per step)\n",
        "        self._update_epsilon()\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance, adapted for SARSA's sequential nature.\"\"\"\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        # 1. Select the initial action A using the current policy (e-greedy)\n",
        "        action = agent.select_action(state)\n",
        "\n",
        "        while True:\n",
        "            # 2. Take action A, observe R, S'\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # 3. Select next action A' using the current policy (e-greedy)\n",
        "            next_action = agent.select_action(next_state)\n",
        "\n",
        "            # 4. Agent performs internal update (uses S, A, R, S', A', done)\n",
        "            agent.step(state, action, reward, next_state, next_action, done)\n",
        "\n",
        "            samp_reward += reward\n",
        "\n",
        "            # 5. Move to the next time step (S=S', A=A')\n",
        "            state = next_state\n",
        "            action = next_action\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        # Monitor progress\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best avg reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.5f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # check if task is solved\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== main notebook entry point ====\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(\"\\nFinal best average reward:\", best_avg_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RobLVuWjezy-",
        "outputId": "4523db00-958a-4ca2-e778-8fbea8cffdf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best average reward 8.64 || ε=0.00100\n",
            "\n",
            "\n",
            "Final best average reward: 8.64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "# === Optimized Epsilon-Decay Q-Learning ===\n",
        "# Strategy: Slower, step-based epsilon decay and slightly tuned hyperparameters\n",
        "# to ensure more thorough exploration of the state space.\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Tabular Q-learning agent for Taxi-v3 with optimized hyperparameters.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Q-table: maps state -> array of action-values (size nA)\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Q-learning hyperparameters\n",
        "        self.alpha = 0.06     # Learning rate: Slightly reduced from 0.1\n",
        "        self.gamma = 0.99     # Discount factor (kept high)\n",
        "\n",
        "        # Epsilon-greedy exploration hyperparameters (CRITICAL CHANGES)\n",
        "        self.epsilon = 1.0        # Initial exploration rate\n",
        "        self.epsilon_min = 0.001  # Minimum exploration rate\n",
        "        # Slower, step-based decay for prolonged exploration: 0.99995 is common\n",
        "        self.epsilon_decay = 0.99998\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Return a greedy action for a given state, breaking ties randomly.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        # All actions with the max Q-value\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        # Break ties randomly among equally good actions\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action using epsilon-greedy policy.\n",
        "\n",
        "        This method now also handles the step-based epsilon decay.\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: uniform over all actions\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            # Exploit: greedy action (with random tie-breaking)\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def _update_epsilon(self):\n",
        "        \"\"\"Decay epsilon once per step (not per episode).\"\"\"\n",
        "        # Step-based epsilon decay schedule\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update and step-based epsilon decay.\"\"\"\n",
        "        # Current Q(s,a)\n",
        "        q_sa = self.Q[state][action]\n",
        "\n",
        "        # Target for Q-learning\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            # Max over next state's action-values\n",
        "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        # TD error\n",
        "        td_error = target - q_sa\n",
        "\n",
        "        # Q-learning update\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # CRITICAL CHANGE: Decay epsilon *per step*\n",
        "        self._update_epsilon()\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance (No change needed here, it's robust).\"\"\"\n",
        "    # initialize average rewards\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    # initialize best average reward\n",
        "    best_avg_reward = -math.inf\n",
        "    # initialize monitor for most recent rewards\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    # for each episode\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        # Gymnasium: reset returns (obs, info)\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            # agent selects an action\n",
        "            action = agent.select_action(state)\n",
        "            # Gymnasium: step returns (obs, reward, terminated, truncated, info)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # agent performs internal updates based on sampled experience\n",
        "            # Epsilon decay is now *inside* agent.step\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            # update the sampled reward\n",
        "            samp_reward += reward\n",
        "            # update the state\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            # get average reward from last window episodes\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        # monitor progress\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best average reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.5f}\", # Showing more digits for epsilon decay\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # check if task is solved (Gym definition used in many examples)\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== main notebook entry point ====\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(\"\\nFinal best average reward:\", best_avg_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TXAA_boiaz2"
      },
      "source": [
        "## Dual Step Decay Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ghqhIA28iehe",
        "outputId": "28950c35-d495-4094-b429-5c864c9afed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best avg reward 8.80 || α=0.146774 | ε=0.00100\n",
            "\n",
            "\n",
            "Final best average reward: 8.8\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "# === Dual Step-Decay Q-Learning ===\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Tabular Q-learning agent with Dual Step-Decay for Alpha and Epsilon.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Q-table: maps state -> array of action-values (size nA). Zero init for stability.\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Q-learning hyperparameters\n",
        "        self.alpha = 0.2      # Learning rate: Start high for fast initial updates\n",
        "        self.alpha_min = 0.01 # Minimum learning rate for fine tuning\n",
        "        # Extremely slow decay per step. Ensures alpha approaches 0.01 smoothly.\n",
        "        self.alpha_decay = 0.999999\n",
        "        self.gamma = 0.99     # Discount factor\n",
        "\n",
        "        # Epsilon-greedy exploration hyperparameters (Per-Step Decay)\n",
        "        self.epsilon = 1.0        # Initial exploration rate\n",
        "        self.epsilon_min = 0.001  # Minimum exploration rate\n",
        "        self.epsilon_decay = 0.99995 # Slower decay per STEP (proven effective)\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Return a greedy action for a given state, breaking ties randomly.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: uniform over all actions\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            # Exploit: greedy action\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def _update_epsilon(self):\n",
        "        \"\"\"Decay epsilon once per step.\"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def _update_alpha(self):\n",
        "        \"\"\"Decay alpha once per step, very slowly.\"\"\"\n",
        "        self.alpha = max(self.alpha_min, self.alpha * self.alpha_decay)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update with dual step-based decay for alpha and epsilon.\n",
        "\n",
        "        Q(s,a) ← Q(s,a) + α [ r + γ max_a' Q(s',a') - Q(s,a) ]\n",
        "        \"\"\"\n",
        "        # Current Q(s,a)\n",
        "        q_sa = self.Q[state][action]\n",
        "\n",
        "        # Target calculation (Q-Learning: uses max Q[next_state])\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        # TD error\n",
        "        td_error = target - q_sa\n",
        "\n",
        "        # Q-learning update using current (and decaying) alpha\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # Update exploration rate (per step)\n",
        "        self._update_epsilon()\n",
        "\n",
        "        # Update learning rate (per step)\n",
        "        self._update_alpha()\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance, adapted back for Q-Learning.\"\"\"\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        # Gymnasium: reset returns (obs, info)\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            # Agent selects action\n",
        "            action = agent.select_action(state)\n",
        "            # Take action, observe R, S'\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Agent performs update and dual decay (uses S, A, R, S', done)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        # Monitor progress (now includes current alpha and epsilon)\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best avg reward {best_avg_reward:.2f} || \"\n",
        "            f\"α={agent.alpha:.6f} | ε={agent.epsilon:.5f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # check if task is solved\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== main notebook entry point ====\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(\"\\nFinal best average reward:\", best_avg_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_7N8BNKx9Q4"
      },
      "source": [
        "# Optimized Q-learning (claude)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "K7Ce0Ym9x-Wr",
        "outputId": "fab1d2de-c7d0-4e78-f94e-43838b827708"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best average reward 4.70 || ε=0.0010 α=0.0050\n",
            "\n",
            "\n",
            "\n",
            "Final best average reward: 4.7\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Optimized Q-learning agent for Taxi-v3 leaderboard.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # CRITICAL: Optimistic initialization encourages exploration\n",
        "        # Initialize Q-values to a positive number rather than 0\n",
        "        self.Q = defaultdict(lambda: np.ones(self.nA) * 2.0)\n",
        "\n",
        "        # --- Hyperparameters (optimized for 9.27+ target) ---\n",
        "        # Learning rate: start higher, decay faster\n",
        "        self.alpha = 0.15         # initial learning rate (higher than your 0.1)\n",
        "        self.alpha_min = 0.005    # lower minimum (more exploitation late)\n",
        "        self.alpha_decay = 0.99995  # slower decay\n",
        "\n",
        "        self.gamma = 0.99         # discount factor (keep this)\n",
        "\n",
        "        # Epsilon: aggressive decay schedule\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.001   # much lower minimum (almost pure exploitation)\n",
        "        self.epsilon_decay = 0.99995  # slower, steadier decay\n",
        "\n",
        "        self.episode = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"ε-greedy action selection (simple and effective).\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - state: the current state of the environment\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "        - action: an integer, compatible with the task's action space\n",
        "        \"\"\"\n",
        "        if np.random.random() > self.epsilon:\n",
        "            # Exploit: choose best known action\n",
        "            return np.argmax(self.Q[state])\n",
        "        else:\n",
        "            # Explore: random action\n",
        "            return np.random.choice(self.nA)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update with per-step parameter decay.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - state: the previous state of the environment\n",
        "        - action: the agent's previous choice of action\n",
        "        - reward: last reward received\n",
        "        - next_state: the current state of the environment\n",
        "        - done: whether the episode is complete (True or False)\n",
        "        \"\"\"\n",
        "        # Q-learning: off-policy TD update\n",
        "        best_next_q = 0.0 if done else np.max(self.Q[next_state])\n",
        "        td_target = reward + self.gamma * best_next_q\n",
        "        td_error = td_target - self.Q[state][action]\n",
        "\n",
        "        # Update Q-value\n",
        "        self.Q[state][action] += self.alpha * td_error\n",
        "\n",
        "        # Decay parameters every step (smoother than per-episode)\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "        self.alpha = max(self.alpha_min, self.alpha * self.alpha_decay)\n",
        "\n",
        "        if done:\n",
        "            self.episode += 1\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance.\n",
        "\n",
        "    Params\n",
        "    ======\n",
        "    - env: instance of Gymnasium's Taxi-v3 environment\n",
        "    - agent: instance of class Agent\n",
        "    - num_episodes: number of episodes of agent-environment interaction\n",
        "    - window: number of episodes to consider when calculating average rewards\n",
        "\n",
        "    Returns\n",
        "    =======\n",
        "    - avg_rewards: deque containing average rewards\n",
        "    - best_avg_reward: largest value in the avg_rewards deque\n",
        "    \"\"\"\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best average reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.4f} α={agent.alpha:.4f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Target threshold\n",
        "        if best_avg_reward >= 9.27:\n",
        "            print(f\"\\n🎯 TARGET REACHED in {i_episode} episodes!\", end=\"\")\n",
        "            break\n",
        "\n",
        "        if i_episode == num_episodes:\n",
        "            print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== main entry point ====\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(\"\\n\\nFinal best average reward:\", best_avg_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ruVu5Okyxvp"
      },
      "source": [
        "Sarsa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qqvNift4yZyJ",
        "outputId": "02053ddc-8d8e-4f47-bcf9-9c37ea2d3988"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best average reward 5.65 || ε=0.0010 α=0.0050\n",
            "\n",
            "\n",
            "\n",
            "Final best average reward: 5.65\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Optimized SARSA agent for Taxi-v3 leaderboard.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # CRITICAL: Optimistic initialization encourages exploration\n",
        "        # Initialize Q-values to a positive number rather than 0\n",
        "        self.Q = defaultdict(lambda: np.ones(self.nA) * 2.0)\n",
        "\n",
        "        # --- Hyperparameters (optimized for 9.27+ target) ---\n",
        "        # Learning rate: start higher, decay faster\n",
        "        self.alpha = 0.15         # initial learning rate\n",
        "        self.alpha_min = 0.005    # lower minimum (more exploitation late)\n",
        "        self.alpha_decay = 0.99995  # slower decay\n",
        "\n",
        "        self.gamma = 0.99         # discount factor\n",
        "\n",
        "        # Epsilon: aggressive decay schedule\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.001   # much lower minimum (almost pure exploitation)\n",
        "        self.epsilon_decay = 0.99995  # slower, steadier decay\n",
        "\n",
        "        self.episode = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"ε-greedy action selection (simple and effective).\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - state: the current state of the environment\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "        - action: an integer, compatible with the task's action space\n",
        "        \"\"\"\n",
        "        if np.random.random() > self.epsilon:\n",
        "            # Exploit: choose best known action\n",
        "            return np.argmax(self.Q[state])\n",
        "        else:\n",
        "            # Explore: random action\n",
        "            return np.random.choice(self.nA)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"SARSA: on-policy TD update.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - state: the previous state of the environment\n",
        "        - action: the agent's previous choice of action\n",
        "        - reward: last reward received\n",
        "        - next_state: the current state of the environment\n",
        "        - done: whether the episode is complete (True or False)\n",
        "        \"\"\"\n",
        "        # SARSA: on-policy update using the action we WILL take\n",
        "        if done:\n",
        "            next_q = 0.0\n",
        "        else:\n",
        "            # Use the action you WILL take (on-policy)\n",
        "            next_action = self.select_action(next_state)\n",
        "            next_q = self.Q[next_state][next_action]\n",
        "\n",
        "        td_target = reward + self.gamma * next_q\n",
        "        td_error = td_target - self.Q[state][action]\n",
        "        self.Q[state][action] += self.alpha * td_error\n",
        "\n",
        "        # Decay parameters every step (smoother than per-episode)\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "        self.alpha = max(self.alpha_min, self.alpha * self.alpha_decay)\n",
        "\n",
        "        if done:\n",
        "            self.episode += 1\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance.\n",
        "\n",
        "    Params\n",
        "    ======\n",
        "    - env: instance of Gymnasium's Taxi-v3 environment\n",
        "    - agent: instance of class Agent\n",
        "    - num_episodes: number of episodes of agent-environment interaction\n",
        "    - window: number of episodes to consider when calculating average rewards\n",
        "\n",
        "    Returns\n",
        "    =======\n",
        "    - avg_rewards: deque containing average rewards\n",
        "    - best_avg_reward: largest value in the avg_rewards deque\n",
        "    \"\"\"\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best average reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.4f} α={agent.alpha:.4f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # Target threshold\n",
        "        if best_avg_reward >= 9.27:\n",
        "            print(f\"\\n🎯 TARGET REACHED in {i_episode} episodes!\", end=\"\")\n",
        "            break\n",
        "\n",
        "        if i_episode == num_episodes:\n",
        "            print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== main entry point ====\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(\"\\n\\nFinal best average reward:\", best_avg_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s3oB6Cfy2NK"
      },
      "source": [
        "More experiments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NvZRCvq0y3hU",
        "outputId": "00b03c6a-616c-48d6-b2ca-888fa9779355"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best avg reward 1.75 || ε=0.1353\n",
            "\n",
            "Final best average reward: 1.75\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"SARSA agent - closely matching Andy Harless's 9.26 solution.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Optimistic initialization - Andy uses 5.0\n",
        "        self.Q = defaultdict(lambda: np.ones(self.nA) * 5.0)\n",
        "\n",
        "        # Learning rate - constant (Andy doesn't decay it)\n",
        "        self.alpha = 0.1\n",
        "\n",
        "        # Discount factor\n",
        "        self.gamma = 0.99\n",
        "\n",
        "        # Epsilon schedule - slow decay per EPISODE\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.9999  # very slow\n",
        "\n",
        "        self.episode = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"ε-greedy action selection.\"\"\"\n",
        "        if np.random.random() > self.epsilon:\n",
        "            return np.argmax(self.Q[state])\n",
        "        else:\n",
        "            return np.random.choice(self.nA)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"SARSA update (on-policy).\"\"\"\n",
        "        # SARSA: use next action from current policy\n",
        "        if done:\n",
        "            next_q = 0.0\n",
        "        else:\n",
        "            next_action = self.select_action(next_state)\n",
        "            next_q = self.Q[next_state][next_action]\n",
        "\n",
        "        # TD update\n",
        "        td_target = reward + self.gamma * next_q\n",
        "        td_error = td_target - self.Q[state][action]\n",
        "        self.Q[state][action] += self.alpha * td_error\n",
        "\n",
        "        # Decay epsilon once per episode\n",
        "        if done:\n",
        "            self.episode += 1\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance.\"\"\"\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        # Print every 100 episodes\n",
        "        if i_episode % 100 == 0:\n",
        "            print(\n",
        "                f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "                f\"Best avg reward {best_avg_reward:.2f} || \"\n",
        "                f\"ε={agent.epsilon:.4f}\",\n",
        "                end=\"\"\n",
        "            )\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        if best_avg_reward >= 9.27:\n",
        "            print(f\"\\n🎯 TARGET REACHED in {i_episode} episodes!\")\n",
        "            break\n",
        "\n",
        "    print(\"\\n\")\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== Main ====\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(f\"Final best average reward: {best_avg_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1unvG8OU1yAw"
      },
      "source": [
        "Another try from claude:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7speGxkL1zXo",
        "outputId": "0ba38b2c-f6e3-4870-94d9-4604501c2d06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0: Avg reward (last 100) = -812.00, ε=0.999\n",
            "Episode 1000: Avg reward (last 100) = -61.45, ε=0.367\n",
            "Episode 2000: Avg reward (last 100) = -0.82, ε=0.135\n",
            "Episode 3000: Avg reward (last 100) = 4.54, ε=0.050\n",
            "Episode 4000: Avg reward (last 100) = 7.31, ε=0.018\n",
            "Episode 5000: Avg reward (last 100) = 8.06, ε=0.010\n",
            "Episode 6000: Avg reward (last 100) = 7.72, ε=0.010\n",
            "Episode 7000: Avg reward (last 100) = 7.28, ε=0.010\n",
            "Episode 8000: Avg reward (last 100) = 7.83, ε=0.010\n",
            "Episode 9000: Avg reward (last 100) = 7.61, ε=0.010\n",
            "\n",
            "Final average (last 100): 7.61\n"
          ]
        }
      ],
      "source": [
        "# SIMPLE BASELINE TEST - Run this first!\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "\n",
        "# Extremely simple Q-learning (no tricks)\n",
        "Q = defaultdict(lambda: np.zeros(6))\n",
        "alpha = 0.1\n",
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "\n",
        "rewards_per_episode = []\n",
        "\n",
        "for episode in range(10000):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    while True:\n",
        "        # Epsilon-greedy\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(Q[state])\n",
        "\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        total_reward += reward\n",
        "\n",
        "        # Q-learning update\n",
        "        best_next = 0 if done else np.max(Q[next_state])\n",
        "        Q[state][action] += alpha * (reward + gamma * best_next - Q[state][action])\n",
        "\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    rewards_per_episode.append(total_reward)\n",
        "    epsilon = max(0.01, epsilon * 0.999)\n",
        "\n",
        "    if episode % 1000 == 0:\n",
        "        avg = np.mean(rewards_per_episode[-100:]) if len(rewards_per_episode) >= 100 else np.mean(rewards_per_episode)\n",
        "        print(f\"Episode {episode}: Avg reward (last 100) = {avg:.2f}, ε={epsilon:.3f}\")\n",
        "\n",
        "print(f\"\\nFinal average (last 100): {np.mean(rewards_per_episode[-100:]):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fob1fLIS13_I",
        "outputId": "966e0109-4ee3-4561-ab1b-5a9bd3153907"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "GRID SEARCH - Testing Multiple Configurations\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Testing: Q-learning (zero init)\n",
            "--------------------------------------------------------------------------------\n",
            "Episode 20000/20000 || Best avg reward 8.52 || ε=0.0100\n",
            "Final: 8.52\n",
            "\n",
            "\n",
            "Testing: Q-learning (optimistic)\n",
            "--------------------------------------------------------------------------------\n",
            "Episode 20000/20000 || Best avg reward 8.69 || ε=0.0100\n",
            "Final: 8.69\n",
            "\n",
            "\n",
            "Testing: Q-learning (fast decay)\n",
            "--------------------------------------------------------------------------------\n",
            "Episode 20000/20000 || Best avg reward 8.78 || ε=0.0010\n",
            "Final: 8.78\n",
            "\n",
            "\n",
            "Testing: SARSA (zero init)\n",
            "--------------------------------------------------------------------------------\n",
            "Episode 20000/20000 || Best avg reward 8.45 || ε=0.0100\n",
            "Final: 8.45\n",
            "\n",
            "\n",
            "Testing: SARSA (optimistic)\n",
            "--------------------------------------------------------------------------------\n",
            "Episode 20000/20000 || Best avg reward 8.23 || ε=0.0100\n",
            "Final: 8.23\n",
            "\n",
            "\n",
            "Testing: SARSA (Andy-style)\n",
            "--------------------------------------------------------------------------------\n",
            "Episode 20000/20000 || Best avg reward 2.28 || ε=0.1353\n",
            "Final: 2.28\n",
            "\n",
            "\n",
            "Testing: Q-learning (high alpha)\n",
            "--------------------------------------------------------------------------------\n",
            "Episode 20000/20000 || Best avg reward 8.48 || ε=0.0100\n",
            "Final: 8.48\n",
            "\n",
            "\n",
            "Testing: SARSA (high alpha)\n",
            "--------------------------------------------------------------------------------\n",
            "Episode 20000/20000 || Best avg reward 8.53 || ε=0.0100\n",
            "Final: 8.53\n",
            "\n",
            "\n",
            "================================================================================\n",
            "SUMMARY - All Results\n",
            "================================================================================\n",
            "1. Q-learning (fast decay): 8.78\n",
            "   Config: {'algorithm': 'q-learning', 'q_init': 0.0, 'alpha': 0.1, 'epsilon_decay': 0.995, 'epsilon_min': 0.001}\n",
            "2. Q-learning (optimistic): 8.69\n",
            "   Config: {'algorithm': 'q-learning', 'q_init': 2.0, 'alpha': 0.1, 'epsilon_decay': 0.999, 'epsilon_min': 0.01}\n",
            "3. SARSA (high alpha): 8.53\n",
            "   Config: {'algorithm': 'sarsa', 'q_init': 0.0, 'alpha': 0.2, 'epsilon_decay': 0.999, 'epsilon_min': 0.01}\n",
            "4. Q-learning (zero init): 8.52\n",
            "   Config: {'algorithm': 'q-learning', 'q_init': 0.0, 'alpha': 0.1, 'epsilon_decay': 0.999, 'epsilon_min': 0.01}\n",
            "5. Q-learning (high alpha): 8.48\n",
            "   Config: {'algorithm': 'q-learning', 'q_init': 0.0, 'alpha': 0.2, 'epsilon_decay': 0.999, 'epsilon_min': 0.01}\n",
            "6. SARSA (zero init): 8.45\n",
            "   Config: {'algorithm': 'sarsa', 'q_init': 0.0, 'alpha': 0.1, 'epsilon_decay': 0.999, 'epsilon_min': 0.01}\n",
            "7. SARSA (optimistic): 8.23\n",
            "   Config: {'algorithm': 'sarsa', 'q_init': 2.0, 'alpha': 0.1, 'epsilon_decay': 0.999, 'epsilon_min': 0.01}\n",
            "8. SARSA (Andy-style): 2.28\n",
            "   Config: {'algorithm': 'sarsa', 'q_init': 5.0, 'alpha': 0.1, 'epsilon_decay': 0.9999, 'epsilon_min': 0.01}\n",
            "\n",
            "🏆 WINNER: Q-learning (fast decay) with 8.78\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6, algorithm='q-learning', **params):\n",
        "        \"\"\"Flexible RL agent for testing different algorithms and hyperparameters.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        - algorithm: 'q-learning' or 'sarsa'\n",
        "        - params: hyperparameters (q_init, alpha, gamma, epsilon_decay, epsilon_min)\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "        self.algorithm = algorithm\n",
        "\n",
        "        # Hyperparameters with defaults\n",
        "        q_init = params.get('q_init', 0.0)\n",
        "        self.Q = defaultdict(lambda: np.ones(self.nA) * q_init)\n",
        "\n",
        "        self.alpha = params.get('alpha', 0.1)\n",
        "        self.gamma = params.get('gamma', 0.99)\n",
        "\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = params.get('epsilon_min', 0.01)\n",
        "        self.epsilon_decay = params.get('epsilon_decay', 0.999)\n",
        "\n",
        "        self.episode = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"ε-greedy action selection.\"\"\"\n",
        "        if np.random.random() > self.epsilon:\n",
        "            return np.argmax(self.Q[state])\n",
        "        else:\n",
        "            return np.random.choice(self.nA)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Update Q-values using specified algorithm.\"\"\"\n",
        "\n",
        "        if self.algorithm == 'q-learning':\n",
        "            # Q-learning: off-policy, use max Q-value\n",
        "            next_q = 0.0 if done else np.max(self.Q[next_state])\n",
        "        else:  # sarsa\n",
        "            # SARSA: on-policy, use actual next action\n",
        "            if done:\n",
        "                next_q = 0.0\n",
        "            else:\n",
        "                next_action = self.select_action(next_state)\n",
        "                next_q = self.Q[next_state][next_action]\n",
        "\n",
        "        # TD update (same for both)\n",
        "        td_target = reward + self.gamma * next_q\n",
        "        td_error = td_target - self.Q[state][action]\n",
        "        self.Q[state][action] += self.alpha * td_error\n",
        "\n",
        "        # Decay epsilon\n",
        "        if done:\n",
        "            self.episode += 1\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100, verbose=True):\n",
        "    \"\"\"Monitor agent's performance.\"\"\"\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        # Print progress\n",
        "        if verbose and i_episode % 100 == 0:\n",
        "            print(\n",
        "                f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "                f\"Best avg reward {best_avg_reward:.2f} || \"\n",
        "                f\"ε={agent.epsilon:.4f}\",\n",
        "                end=\"\"\n",
        "            )\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        if best_avg_reward >= 9.27:\n",
        "            if verbose:\n",
        "                print(f\"\\n🎯 TARGET REACHED in {i_episode} episodes!\")\n",
        "            break\n",
        "\n",
        "    if verbose:\n",
        "        print()\n",
        "    return best_avg_reward\n",
        "\n",
        "\n",
        "def grid_search():\n",
        "    \"\"\"Test different hyperparameter combinations.\"\"\"\n",
        "\n",
        "    configs = [\n",
        "        # Classic Q-learning variants\n",
        "        {'name': 'Q-learning (zero init)', 'algorithm': 'q-learning', 'q_init': 0.0,\n",
        "         'alpha': 0.1, 'epsilon_decay': 0.999, 'epsilon_min': 0.01},\n",
        "\n",
        "        {'name': 'Q-learning (optimistic)', 'algorithm': 'q-learning', 'q_init': 2.0,\n",
        "         'alpha': 0.1, 'epsilon_decay': 0.999, 'epsilon_min': 0.01},\n",
        "\n",
        "        {'name': 'Q-learning (fast decay)', 'algorithm': 'q-learning', 'q_init': 0.0,\n",
        "         'alpha': 0.1, 'epsilon_decay': 0.995, 'epsilon_min': 0.001},\n",
        "\n",
        "        # SARSA variants\n",
        "        {'name': 'SARSA (zero init)', 'algorithm': 'sarsa', 'q_init': 0.0,\n",
        "         'alpha': 0.1, 'epsilon_decay': 0.999, 'epsilon_min': 0.01},\n",
        "\n",
        "        {'name': 'SARSA (optimistic)', 'algorithm': 'sarsa', 'q_init': 2.0,\n",
        "         'alpha': 0.1, 'epsilon_decay': 0.999, 'epsilon_min': 0.01},\n",
        "\n",
        "        {'name': 'SARSA (Andy-style)', 'algorithm': 'sarsa', 'q_init': 5.0,\n",
        "         'alpha': 0.1, 'epsilon_decay': 0.9999, 'epsilon_min': 0.01},\n",
        "\n",
        "        # High learning rate variants\n",
        "        {'name': 'Q-learning (high alpha)', 'algorithm': 'q-learning', 'q_init': 0.0,\n",
        "         'alpha': 0.2, 'epsilon_decay': 0.999, 'epsilon_min': 0.01},\n",
        "\n",
        "        {'name': 'SARSA (high alpha)', 'algorithm': 'sarsa', 'q_init': 0.0,\n",
        "         'alpha': 0.2, 'epsilon_decay': 0.999, 'epsilon_min': 0.01},\n",
        "    ]\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"GRID SEARCH - Testing Multiple Configurations\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for config in configs:\n",
        "        name = config.pop('name')\n",
        "        print(f\"\\n\\nTesting: {name}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        env = gym.make(\"Taxi-v3\")\n",
        "        agent = Agent(nA=env.action_space.n, **config)\n",
        "        best_reward = interact(env, agent, num_episodes=20000, window=100, verbose=True)\n",
        "\n",
        "        results.append({\n",
        "            'name': name,\n",
        "            'config': config,\n",
        "            'best_reward': best_reward\n",
        "        })\n",
        "\n",
        "        print(f\"Final: {best_reward:.2f}\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\\n\" + \"=\" * 80)\n",
        "    print(\"SUMMARY - All Results\")\n",
        "    print(\"=\" * 80)\n",
        "    results.sort(key=lambda x: x['best_reward'], reverse=True)\n",
        "\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"{i}. {result['name']}: {result['best_reward']:.2f}\")\n",
        "        print(f\"   Config: {result['config']}\")\n",
        "\n",
        "    print(\"\\n🏆 WINNER:\", results[0]['name'], f\"with {results[0]['best_reward']:.2f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Run grid search\n",
        "results = grid_search()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rajGoPHj1VCJ"
      },
      "source": [
        "Expected sarsa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Su9eykJv3TWy",
        "outputId": "3f0e19b7-e9b1-4456-f673-bf28dfa931c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best average reward 8.71\n",
            "\n",
            "\n",
            "Final best average reward: 8.71\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym  # updated: use gymnasium instead of gym\n",
        "\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Initialize agent.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Hyperparameters for Expected SARSA\n",
        "        self.alpha = 0.1      # learning rate\n",
        "        self.gamma = 0.99     # discount factor\n",
        "\n",
        "        # Epsilon-greedy parameters\n",
        "        self.epsilon = 1.0        # initial exploration rate\n",
        "        self.epsilon_min = 0.01   # minimum exploration rate\n",
        "        self.epsilon_decay = 0.9999  # decay rate per step\n",
        "\n",
        "        self.episode_count = 0\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Given the state, select an action using epsilon-greedy policy.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - state: the current state of the environment\n",
        "\n",
        "        Returns\n",
        "        =======\n",
        "        - action: an integer, compatible with the task's action space\n",
        "        \"\"\"\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: choose random action\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            # Exploit: choose best action based on current Q-values\n",
        "            return np.argmax(self.Q[state])\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Update the agent's knowledge using Expected SARSA update rule.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - state: the previous state of the environment\n",
        "        - action: the agent's previous choice of action\n",
        "        - reward: last reward received\n",
        "        - next_state: the current state of the environment\n",
        "        - done: whether the episode is complete (True or False)\n",
        "        \"\"\"\n",
        "        # Expected SARSA target:\n",
        "        # target = r + γ * E_{a' ~ π(·|s')} [ Q(s', a') ]\n",
        "        # where π is the current ε-greedy policy.\n",
        "\n",
        "        if done:\n",
        "            # Terminal state: no future rewards\n",
        "            target = reward\n",
        "        else:\n",
        "            q_next = self.Q[next_state]\n",
        "\n",
        "            # current greedy action in next_state\n",
        "            best_action = np.argmax(q_next)\n",
        "\n",
        "            # ε-greedy policy over next_state\n",
        "            policy_probs = np.ones(self.nA) * (self.epsilon / self.nA)\n",
        "            policy_probs[best_action] += (1.0 - self.epsilon)\n",
        "\n",
        "            # Expected value under current policy\n",
        "            expected_q_next = np.dot(policy_probs, q_next)\n",
        "\n",
        "            target = reward + self.gamma * expected_q_next\n",
        "\n",
        "        # Update Q-value towards the target\n",
        "        self.Q[state][action] += self.alpha * (target - self.Q[state][action])\n",
        "\n",
        "        # Decay epsilon after each step (same as before so you can compare)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance.\n",
        "\n",
        "    Params\n",
        "    ======\n",
        "    - env: instance of Gymnasium's Taxi-v3 environment\n",
        "    - agent: instance of class Agent\n",
        "    - num_episodes: number of episodes of agent-environment interaction\n",
        "    - window: number of episodes to consider when calculating average rewards\n",
        "\n",
        "    Returns\n",
        "    =======\n",
        "    - avg_rewards: deque containing average rewards\n",
        "    - best_avg_reward: largest value in the avg_rewards deque\n",
        "    \"\"\"\n",
        "    # initialize average rewards\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    # initialize best average reward\n",
        "    best_avg_reward = -math.inf\n",
        "    # initialize monitor for most recent rewards\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    # for each episode\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        # Gymnasium: reset returns (obs, info)\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            # agent selects an action\n",
        "            action = agent.select_action(state)\n",
        "            # Gymnasium: step returns (obs, reward, terminated, truncated, info)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # agent performs internal updates based on sampled experience\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            # update the sampled reward\n",
        "            samp_reward += reward\n",
        "            # update the state\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            # get average reward from last window episodes\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        # monitor progress\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || Best average reward {best_avg_reward:.2f}\",\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # check if task is solved (Gym definition used in many examples)\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ==== main notebook entry point ====\n",
        "# create environment and agent, then run training\n",
        "env = gym.make(\"Taxi-v3\")  # updated: Taxi-v3 with Gymnasium\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(\"\\nFinal best average reward:\", best_avg_reward)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmyjJgwjhJcS"
      },
      "source": [
        "## Pure Sarsa:\n",
        "\n",
        "- Q_init = 5.0 (very optimistic)\n",
        "- alpha = 0.1 (constant, NO decay)\n",
        "- gamma = 0.99\n",
        "- epsilon: 1.0 → 0.01, decay = 0.9999 per episode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O84Xk_0-hUjw",
        "outputId": "90540250-8b61-4500-b06b-f7cafcd3b42b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best avg 1.66 || ε=0.1353\n",
            "Final: 1.66\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Pure SARSA - exact clone of Andy Harless's winning solution.\"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Very optimistic initialization\n",
        "        self.Q = defaultdict(lambda: np.ones(self.nA) * 5.0)\n",
        "\n",
        "        # Constant learning rate (no decay!)\n",
        "        self.alpha = 0.1\n",
        "\n",
        "        # High discount factor\n",
        "        self.gamma = 0.99\n",
        "\n",
        "        # Epsilon decay per EPISODE (not step!)\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.9999  # Very slow, per episode\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Simple epsilon-greedy.\"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            return np.argmax(self.Q[state])\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"SARSA update (on-policy).\"\"\"\n",
        "\n",
        "        # SARSA: use the action you will actually take next\n",
        "        if done:\n",
        "            next_q = 0.0\n",
        "        else:\n",
        "            next_action = self.select_action(next_state)\n",
        "            next_q = self.Q[next_state][next_action]\n",
        "\n",
        "        # TD update\n",
        "        target = reward + self.gamma * next_q\n",
        "        self.Q[state][action] += self.alpha * (target - self.Q[state][action])\n",
        "\n",
        "        # Decay epsilon ONCE per episode\n",
        "        if done:\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Standard interaction loop.\"\"\"\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        if i_episode % 100 == 0:\n",
        "            print(\n",
        "                f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "                f\"Best avg {best_avg_reward:.2f} || \"\n",
        "                f\"ε={agent.epsilon:.4f}\",\n",
        "                end=\"\"\n",
        "            )\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        if best_avg_reward >= 9.27:\n",
        "            print(f\"\\n🎯 REACHED in {i_episode} episodes!\")\n",
        "            break\n",
        "\n",
        "    print()\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(f\"Final: {best_avg_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ621p1chVqa"
      },
      "source": [
        "## Q-Learning with Episode-Based Decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MfzNCQsshZgJ",
        "outputId": "4f59e5c0-f48a-405b-c51e-9bd277b528eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best avg 8.41 || ε=0.0100\n",
            "Final: 8.41\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Q-learning with EPISODE-based epsilon decay.\"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Moderate optimistic initialization\n",
        "        self.Q = defaultdict(lambda: np.ones(self.nA) * 2.0)\n",
        "\n",
        "        self.alpha = 0.1\n",
        "        self.gamma = 0.99\n",
        "\n",
        "        # Episode-based epsilon decay\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.9995  # Faster than Andy's, per episode\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Epsilon-greedy with tie-breaking.\"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            q_vals = self.Q[state]\n",
        "            return np.random.choice(np.flatnonzero(q_vals == np.max(q_vals)))\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update.\"\"\"\n",
        "\n",
        "        # Q-learning: off-policy, use max\n",
        "        next_q = 0.0 if done else np.max(self.Q[next_state])\n",
        "        target = reward + self.gamma * next_q\n",
        "        self.Q[state][action] += self.alpha * (target - self.Q[state][action])\n",
        "\n",
        "        # Decay epsilon ONCE per episode\n",
        "        if done:\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        if i_episode % 100 == 0:\n",
        "            print(f\"\\rEpisode {i_episode}/{num_episodes} || Best avg {best_avg_reward:.2f} || ε={agent.epsilon:.4f}\", end=\"\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        if best_avg_reward >= 9.27:\n",
        "            print(f\"\\n🎯 REACHED in {i_episode} episodes!\")\n",
        "            break\n",
        "\n",
        "    print()\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(f\"Final: {best_avg_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdKj1GaJhcjo"
      },
      "source": [
        "## Hybrid - Your Best Base + Episode Decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xCktOBqZheAU",
        "outputId": "367c00c2-d21a-4ff3-bb61-67eee9a24793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 20000/20000 || Best avg 8.65 || ε=0.00100\n",
            "Final: 8.65\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6):\n",
        "        \"\"\"Take your working 8.96 solution but switch to EPISODE-based decay.\"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Your working settings\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))  # Conservative init that worked\n",
        "        self.alpha = 0.08  # Your value that worked\n",
        "        self.gamma = 0.99\n",
        "\n",
        "        # CRITICAL CHANGE: Episode-based epsilon decay\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.001\n",
        "        self.epsilon_decay = 0.9993  # Tuned for episode-based (faster than 0.9999)\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Your tie-breaking implementation.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning - same as your working version.\"\"\"\n",
        "        q_sa = self.Q[state][action]\n",
        "        target = reward if done else reward + self.gamma * np.max(self.Q[next_state])\n",
        "        self.Q[state][action] = q_sa + self.alpha * (target - q_sa)\n",
        "\n",
        "        # CHANGE: Decay epsilon per EPISODE, not per step\n",
        "        if done:\n",
        "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        print(f\"\\rEpisode {i_episode}/{num_episodes} || Best avg {best_avg_reward:.2f} || ε={agent.epsilon:.5f}\", end=\"\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        if best_avg_reward >= 9.27:\n",
        "            print(f\"\\n🎯 REACHED in {i_episode} episodes!\")\n",
        "            break\n",
        "\n",
        "    print()\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "agent = Agent(nA=env.action_space.n)\n",
        "avg_rewards, best_avg_reward = interact(env, agent)\n",
        "print(f\"Final: {best_avg_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aoy_TsQudmn"
      },
      "source": [
        "## Manus Version 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ej2jHRPBuc7O",
        "outputId": "8075440c-fbcb-46fe-9b79-9fffa4d1405c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running Experiment: α=0.06, γ=0.995, ε_decay=0.99998 ---\n",
            "Episode 30000/30000 || Best average reward 8.80 || ε=0.00100\n",
            "\n",
            "Final best average reward: 8.8000\n",
            "\n",
            "Final best average reward from optimized configuration: 8.8000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "# === Optimized Epsilon-Decay Q-Learning ===\n",
        "# Strategy: Slower, step-based epsilon decay and slightly tuned hyperparameters\n",
        "# to ensure more thorough exploration of the state space.\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6, alpha=0.08, gamma=0.99, epsilon_decay=0.99995):\n",
        "        \"\"\"Tabular Q-learning agent for Taxi-v3 with optimized hyperparameters.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        - alpha: learning rate\n",
        "        - gamma: discount factor\n",
        "        - epsilon_decay: step-based decay rate for epsilon\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Q-table: maps state -> array of action-values (size nA)\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Q-learning hyperparameters\n",
        "        self.alpha = alpha     # Learning rate\n",
        "        self.gamma = gamma     # Discount factor\n",
        "\n",
        "        # Epsilon-greedy exploration hyperparameters (CRITICAL CHANGES)\n",
        "        self.epsilon = 1.0        # Initial exploration rate\n",
        "        self.epsilon_min = 0.001  # Minimum exploration rate\n",
        "        # Slower, step-based decay for prolonged exploration\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Return a greedy action for a given state, breaking ties randomly.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        # All actions with the max Q-value\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        # Break ties randomly among equally good actions\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action using epsilon-greedy policy.\n",
        "\n",
        "        This method now also handles the step-based epsilon decay.\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: uniform over all actions\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            # Exploit: greedy action (with random tie-breaking)\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def _update_epsilon(self):\n",
        "        \"\"\"Decay epsilon once per step (not per episode).\"\"\"\n",
        "        # Step-based epsilon decay schedule\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update and step-based epsilon decay.\"\"\"\n",
        "        # Current Q(s,a)\n",
        "        q_sa = self.Q[state][action]\n",
        "\n",
        "        # Target for Q-learning\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            # Max over next state's action-values\n",
        "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        # TD error\n",
        "        td_error = target - q_sa\n",
        "\n",
        "        # Q-learning update\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # CRITICAL CHANGE: Decay epsilon *per step*\n",
        "        self._update_epsilon()\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance (No change needed here, it's robust).\"\"\"\n",
        "    # initialize average rewards\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    # initialize best average reward\n",
        "    best_avg_reward = -math.inf\n",
        "    # initialize monitor for most recent rewards\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    # for each episode\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        # Gymnasium: reset returns (obs, info)\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            # agent selects an action\n",
        "            action = agent.select_action(state)\n",
        "            # Gymnasium: step returns (obs, reward, terminated, truncated, info)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # agent performs internal updates based on sampled experience\n",
        "            # Epsilon decay is now *inside* agent.step\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            # update the sampled reward\n",
        "            samp_reward += reward\n",
        "            # update the state\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            # get average reward from last window episodes\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        # monitor progress\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best average reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.5f}\", # Showing more digits for epsilon decay\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # check if task is solved (Gym definition used in many examples)\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "def run_experiment(alpha, gamma, epsilon_decay, num_episodes=20000):\n",
        "    \"\"\"Runs a single experiment with specified hyperparameters.\"\"\"\n",
        "    env = gym.make(\"Taxi-v3\")\n",
        "    agent = Agent(nA=env.action_space.n, alpha=alpha, gamma=gamma, epsilon_decay=epsilon_decay)\n",
        "    print(f\"\\n--- Running Experiment: α={alpha}, γ={gamma}, ε_decay={epsilon_decay} ---\")\n",
        "    _, best_avg_reward = interact(env, agent, num_episodes=num_episodes)\n",
        "    print(f\"Final best average reward: {best_avg_reward:.4f}\")\n",
        "    return best_avg_reward\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Optimized Configuration:\n",
        "    # - Alpha (Learning Rate): 0.06 (Slightly lower than 0.08 for better convergence)\n",
        "    # - Gamma (Discount Factor): 0.995 (Slightly higher than 0.99 to value future rewards more)\n",
        "    # - Epsilon Decay: 0.99998 (Slower decay to ensure exploration lasts longer)\n",
        "    # - Episodes: 30000 (Increased to allow the slower decay to converge)\n",
        "\n",
        "    best_avg_reward = run_experiment(alpha=0.06, gamma=0.995, epsilon_decay=0.99998, num_episodes=30000)\n",
        "    print(f\"\\nFinal best average reward from optimized configuration: {best_avg_reward:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN_30YiRupjk"
      },
      "source": [
        "## Version 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "we8YcgiHutyz",
        "outputId": "fe05c130-840a-44b0-b8a1-50a1d0d485b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running Experiment: α=0.08, γ=0.99, ε_decay=0.99995 ---\n",
            "Episode 20000/20000 || Best average reward 9.01 || ε=0.00100\n",
            "\n",
            "Final best average reward: 9.0100\n",
            "\n",
            "Final best average reward from baseline configuration: 9.0100\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "# === Optimized Epsilon-Decay Q-Learning ===\n",
        "# Strategy: Slower, step-based epsilon decay and slightly tuned hyperparameters\n",
        "# to ensure more thorough exploration of the state space.\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6, alpha=0.08, gamma=0.99, epsilon_decay=0.99995):\n",
        "        \"\"\"Tabular Q-learning agent for Taxi-v3 with optimized hyperparameters.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        - alpha: learning rate\n",
        "        - gamma: discount factor\n",
        "        - epsilon_decay: step-based decay rate for epsilon\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Q-table: maps state -> array of action-values (size nA)\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Q-learning hyperparameters\n",
        "        self.alpha = alpha     # Learning rate\n",
        "        self.gamma = gamma     # Discount factor\n",
        "\n",
        "        # Epsilon-greedy exploration hyperparameters (CRITICAL CHANGES)\n",
        "        self.epsilon = 1.0        # Initial exploration rate\n",
        "        self.epsilon_min = 0.001  # Minimum exploration rate\n",
        "        # Slower, step-based decay for prolonged exploration\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Return a greedy action for a given state, breaking ties randomly.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        # All actions with the max Q-value\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        # Break ties randomly among equally good actions\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action using epsilon-greedy policy.\n",
        "\n",
        "        This method now also handles the step-based epsilon decay.\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: uniform over all actions\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            # Exploit: greedy action (with random tie-breaking)\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def _update_epsilon(self):\n",
        "        \"\"\"Decay epsilon once per step (not per episode).\"\"\"\n",
        "        # Step-based epsilon decay schedule\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update and step-based epsilon decay.\"\"\"\n",
        "        # Current Q(s,a)\n",
        "        q_sa = self.Q[state][action]\n",
        "\n",
        "        # Target for Q-learning\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            # Max over next state's action-values\n",
        "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        # TD error\n",
        "        td_error = target - q_sa\n",
        "\n",
        "        # Q-learning update\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # CRITICAL CHANGE: Decay epsilon *per step*\n",
        "        self._update_epsilon()\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance (No change needed here, it's robust).\"\"\"\n",
        "    # initialize average rewards\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    # initialize best average reward\n",
        "    best_avg_reward = -math.inf\n",
        "    # initialize monitor for most recent rewards\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    # for each episode\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        # Gymnasium: reset returns (obs, info)\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            # agent selects an action\n",
        "            action = agent.select_action(state)\n",
        "            # Gymnasium: step returns (obs, reward, terminated, truncated, info)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # agent performs internal updates based on sampled experience\n",
        "            # Epsilon decay is now *inside* agent.step\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            # update the sampled reward\n",
        "            samp_reward += reward\n",
        "            # update the state\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            # get average reward from last window episodes\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        # monitor progress\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best average reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.5f}\", # Showing more digits for epsilon decay\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # check if task is solved (Gym definition used in many examples)\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "def run_experiment(alpha, gamma, epsilon_decay, num_episodes=20000):\n",
        "    \"\"\"Runs a single experiment with specified hyperparameters.\"\"\"\n",
        "    env = gym.make(\"Taxi-v3\")\n",
        "    agent = Agent(nA=env.action_space.n, alpha=alpha, gamma=gamma, epsilon_decay=epsilon_decay)\n",
        "    print(f\"\\n--- Running Experiment: α={alpha}, γ={gamma}, ε_decay={epsilon_decay} ---\")\n",
        "    _, best_avg_reward = interact(env, agent, num_episodes=num_episodes)\n",
        "    print(f\"Final best average reward: {best_avg_reward:.4f}\")\n",
        "    return best_avg_reward\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Replicating the user's best run to establish a baseline.\n",
        "    best_avg_reward = run_experiment(alpha=0.08, gamma=0.99, epsilon_decay=0.99995, num_episodes=20000)\n",
        "    print(f\"\\nFinal best average reward from baseline configuration: {best_avg_reward:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCYfLr0quyiA"
      },
      "source": [
        "## Version 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r4idNaxlu0la",
        "outputId": "86122b95-358f-4c4f-e13d-8eba376301d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Running Experiment: α=0.08, γ=0.99, ε_decay=0.99998 ---\n",
            "Episode 30000/30000 || Best average reward 8.73 || ε=0.00100\n",
            "\n",
            "Final best average reward: 8.7300\n",
            "Final best average reward from best configuration: 8.7300\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import sys\n",
        "from collections import defaultdict, deque\n",
        "import gymnasium as gym\n",
        "\n",
        "# === Optimized Epsilon-Decay Q-Learning ===\n",
        "# Strategy: Slower, step-based epsilon decay and slightly tuned hyperparameters\n",
        "# to ensure more thorough exploration of the state space.\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, nA=6, alpha=0.08, gamma=0.99, epsilon_decay=0.99995):\n",
        "        \"\"\"Tabular Q-learning agent for Taxi-v3 with optimized hyperparameters.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "        - nA: number of actions available to the agent\n",
        "        - alpha: learning rate\n",
        "        - gamma: discount factor\n",
        "        - epsilon_decay: step-based decay rate for epsilon\n",
        "        \"\"\"\n",
        "        self.nA = nA\n",
        "\n",
        "        # Q-table: maps state -> array of action-values (size nA)\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        # Q-learning hyperparameters\n",
        "        self.alpha = alpha     # Learning rate\n",
        "        self.gamma = gamma     # Discount factor\n",
        "\n",
        "        # Epsilon-greedy exploration hyperparameters (CRITICAL CHANGES)\n",
        "        self.epsilon = 1.0        # Initial exploration rate\n",
        "        self.epsilon_min = 0.001  # Minimum exploration rate\n",
        "        # Slower, step-based decay for prolonged exploration\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "    def _greedy_action(self, state):\n",
        "        \"\"\"Return a greedy action for a given state, breaking ties randomly.\"\"\"\n",
        "        q_vals = self.Q[state]\n",
        "        max_q = np.max(q_vals)\n",
        "        # All actions with the max Q-value\n",
        "        best_actions = np.flatnonzero(q_vals == max_q)\n",
        "        # Break ties randomly among equally good actions\n",
        "        return np.random.choice(best_actions)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select an action using epsilon-greedy policy.\n",
        "\n",
        "        This method now also handles the step-based epsilon decay.\n",
        "        \"\"\"\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: uniform over all actions\n",
        "            return np.random.choice(self.nA)\n",
        "        else:\n",
        "            # Exploit: greedy action (with random tie-breaking)\n",
        "            return self._greedy_action(state)\n",
        "\n",
        "    def _update_epsilon(self):\n",
        "        \"\"\"Decay epsilon once per step (not per episode).\"\"\"\n",
        "        # Step-based epsilon decay schedule\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Q-learning update and step-based epsilon decay.\"\"\"\n",
        "        # Current Q(s,a)\n",
        "        q_sa = self.Q[state][action]\n",
        "\n",
        "        # Target for Q-learning\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            # Max over next state's action-values\n",
        "            target = reward + self.gamma * np.max(self.Q[next_state])\n",
        "\n",
        "        # TD error\n",
        "        td_error = target - q_sa\n",
        "\n",
        "        # Q-learning update\n",
        "        self.Q[state][action] = q_sa + self.alpha * td_error\n",
        "\n",
        "        # CRITICAL CHANGE: Decay epsilon *per step*\n",
        "        self._update_epsilon()\n",
        "\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100):\n",
        "    \"\"\"Monitor agent's performance (No change needed here, it's robust).\"\"\"\n",
        "    # initialize average rewards\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    # initialize best average reward\n",
        "    best_avg_reward = -math.inf\n",
        "    # initialize monitor for most recent rewards\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    # for each episode\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        # Gymnasium: reset returns (obs, info)\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            # agent selects an action\n",
        "            action = agent.select_action(state)\n",
        "            # Gymnasium: step returns (obs, reward, terminated, truncated, info)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # agent performs internal updates based on sampled experience\n",
        "            # Epsilon decay is now *inside* agent.step\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            # update the sampled reward\n",
        "            samp_reward += reward\n",
        "            # update the state\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            # get average reward from last window episodes\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        # monitor progress\n",
        "        print(\n",
        "            f\"\\rEpisode {i_episode}/{num_episodes} || \"\n",
        "            f\"Best average reward {best_avg_reward:.2f} || \"\n",
        "            f\"ε={agent.epsilon:.5f}\", # Showing more digits for epsilon decay\n",
        "            end=\"\"\n",
        "        )\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # check if task is solved (Gym definition used in many examples)\n",
        "        # We will use a higher threshold for the final check, but keep the original for comparison\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\", end=\"\")\n",
        "            break\n",
        "\n",
        "    if i_episode == num_episodes:\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "def run_experiment(alpha, gamma, epsilon_decay, num_episodes=20000):\n",
        "    \"\"\"Runs a single experiment with specified hyperparameters.\"\"\"\n",
        "    # Use the original Taxi-v3 environment\n",
        "    env = gym.make(\"Taxi-v3\")\n",
        "    agent = Agent(nA=env.action_space.n, alpha=alpha, gamma=gamma, epsilon_decay=epsilon_decay)\n",
        "    print(f\"\\n--- Running Experiment: α={alpha}, γ={gamma}, ε_decay={epsilon_decay} ---\")\n",
        "    _, best_avg_reward = interact(env, agent, num_episodes=num_episodes)\n",
        "    print(f\"Final best average reward: {best_avg_reward:.4f}\")\n",
        "    return best_avg_reward\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Based on user insights:\n",
        "    # - Fixed alpha=0.08 is good. Let's try a slightly lower one for better convergence.\n",
        "    # - Gamma=0.99 is high, which is good for long-term rewards.\n",
        "    # - Step-based decay is critical. Let's try a slower decay rate.\n",
        "\n",
        "    # Configuration 1: Baseline (similar to user's best result)\n",
        "    # alpha=0.08, gamma=0.99, epsilon_decay=0.99995\n",
        "    # run_experiment(alpha=0.08, gamma=0.99, epsilon_decay=0.99995)\n",
        "\n",
        "    # Configuration 2: Slower decay for more exploration\n",
        "    # The agent reaches epsilon_min=0.001 after about 138,000 steps with 0.99995 decay.\n",
        "    # A typical episode is around 13 steps. 20000 episodes * 13 steps/episode = 260,000 steps.\n",
        "    # Let's try a decay that ensures exploration lasts longer than 20,000 episodes.\n",
        "    # 0.99998: (0.001/1.0) = 0.99998^x -> x = log(0.001)/log(0.99998) ~ 345,000 steps\n",
        "    # This should keep epsilon active for the full 20,000 episodes.\n",
        "\n",
        "    # Configuration 2: Slower decay, same alpha\n",
        "    # best_avg_reward_2 = run_experiment(alpha=0.08, gamma=0.99, epsilon_decay=0.99998)\n",
        "\n",
        "    # Configuration 3: Slower decay, slightly lower alpha for better convergence\n",
        "    # best_avg_reward_3 = run_experiment(alpha=0.06, gamma=0.99, epsilon_decay=0.99998)\n",
        "\n",
        "    # Configuration 4: Slower decay, slightly higher gamma (more future-focused)\n",
        "    # best_avg_reward_4 = run_experiment(alpha=0.08, gamma=0.995, epsilon_decay=0.99998)\n",
        "\n",
        "    # We will start with the most promising configuration based on the user's insights and our analysis:\n",
        "    # Slower decay (0.99998) and the proven alpha (0.08).\n",
        "    best_avg_reward_final = run_experiment(alpha=0.08, gamma=0.99, epsilon_decay=0.99998, num_episodes=30000)\n",
        "\n",
        "    # If the first run is not enough, we can try the other configurations in subsequent steps.\n",
        "    # Increasing episodes to 30,000 to give the slower decay more time to converge.\n",
        "    print(f\"Final best average reward from best configuration: {best_avg_reward_final:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBCidSGTxDUw"
      },
      "source": [
        "Andy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yO0SfCm_yCjR",
        "outputId": "7d20251b-b503-403d-a80a-7b4589928900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Begin Taxi-v3 learning...\n",
            "Episode 10000/150000 || epsilon=0.6035056, Best avg reward -81.52\n",
            "Episode 20000/150000 || epsilon=0.3660446, Best avg reward -20.1\n",
            "Episode 30000/150000 || epsilon=0.2220173, Best avg reward -4.68\n",
            "Episode 40000/150000 || epsilon=0.1346603, Best avg reward 2.71\n",
            "Episode 50000/150000 || epsilon=0.0816756, Best avg reward 5.14\n",
            "Episode 60000/150000 || epsilon=0.0495388, Best avg reward 6.59\n",
            "Episode 70000/150000 || epsilon=0.0300468, Best avg reward 7.67\n",
            "Episode 80000/150000 || epsilon=0.0182243, Best avg reward 8.05\n",
            "Episode 90000/150000 || epsilon=0.0110536, Best avg reward 8.34\n",
            "Episode 100000/150000 || epsilon=0.0067043, Best avg reward 8.7\n",
            "Episode 110000/150000 || epsilon=0.0040664, Best avg reward 8.7\n",
            "Episode 120000/150000 || epsilon=0.0024664, Best avg reward 8.7\n",
            "Episode 130000/150000 || epsilon=0.0014959, Best avg reward 8.71\n",
            "Episode 140000/150000 || epsilon=0.0009073, Best avg reward 9.0\n",
            "Episode 150000/150000 || epsilon=0.0005503, Best avg reward 9.0\n",
            "Run 3/1, avg so far = 9.0\n",
            "\n",
            "Local seed: 80\n",
            "Average: 9.0\n",
            "Median: 9.0\n",
            "[9.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "import sys, math, random\n",
        "import gymnasium as gym  # <-- IMPORTANT: Gymnasium\n",
        "\n",
        "\n",
        "# ============================\n",
        "#        AGENT.PY\n",
        "# ============================\n",
        "\n",
        "class Agent:\n",
        "\n",
        "    def __init__(self, nA=6, alpha=.75, gamma=1, beta=.8, c1=0, c2=0,\n",
        "                 get_epsilon=lambda i: .8*i**.999,\n",
        "                 get_alpha=None, get_gamma=None, get_beta=None,\n",
        "                 get_c1=None, get_c2=None):\n",
        "\n",
        "        self.alpha_init = alpha\n",
        "        self.gamma_init = gamma\n",
        "        self.beta_init = beta\n",
        "        self.c1_init = c1\n",
        "        self.c2_init = c2\n",
        "\n",
        "        self.get_epsilon = get_epsilon\n",
        "        self.get_alpha = (lambda i:self.alpha_init) if get_alpha is None else get_alpha\n",
        "        self.get_gamma = (lambda i:self.gamma_init) if get_gamma is None else get_gamma\n",
        "        self.get_beta = (lambda i:self.beta_init) if get_beta is None else get_beta\n",
        "        self.get_c1 = (lambda i:self.c1_init) if get_c1 is None else get_c1\n",
        "        self.get_c2 = (lambda i:self.c2_init) if get_c2 is None else get_c2\n",
        "\n",
        "        self.nA = nA\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "        self.recent = defaultdict(lambda: np.zeros(self.nA))\n",
        "\n",
        "        self.epsilon = self.get_epsilon(0)\n",
        "        self.alpha = self.get_alpha(0)\n",
        "        self.gamma = self.get_gamma(0)\n",
        "        self.beta = self.get_beta(0)\n",
        "        self.c1 = self.get_c1(0)\n",
        "        self.c2 = self.get_c2(0)\n",
        "        self.i_episode = 0\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "\n",
        "        if state not in self.Q:\n",
        "            return np.random.choice(self.nA)\n",
        "\n",
        "        q = np.asarray(self.Q[state])\n",
        "        r = np.asarray(self.recent[state])\n",
        "\n",
        "        p = self.softmax(q*self.c1 - r*self.c2)\n",
        "\n",
        "        greedy_action = q.argmax()\n",
        "        random_action = np.random.choice(self.nA, p=p)\n",
        "\n",
        "        return np.random.choice([random_action, greedy_action],\n",
        "                                p=[self.epsilon, 1-self.epsilon])\n",
        "\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "        greedy_action = self.Q[next_state].argmax()\n",
        "\n",
        "        self.Q[state][action] += self.alpha * (\n",
        "            reward + self.gamma * self.Q[next_state][greedy_action] - self.Q[state][action]\n",
        "        )\n",
        "\n",
        "        self.recent[state][action] += 1\n",
        "\n",
        "        if done:\n",
        "            for st in self.recent:\n",
        "                self.recent[st] = [count * self.beta for count in self.recent[st]]\n",
        "\n",
        "            self.i_episode += 1\n",
        "            self.epsilon = self.get_epsilon(self.i_episode)\n",
        "            self.alpha = self.get_alpha(self.i_episode)\n",
        "            self.gamma = self.get_gamma(self.i_episode)\n",
        "            self.beta = self.get_beta(self.i_episode)\n",
        "            self.c1 = self.get_c1(self.i_episode)\n",
        "            self.c2 = self.get_c2(self.i_episode)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(a):\n",
        "        e = np.exp(a - np.max(a))\n",
        "        return e / e.sum()\n",
        "\n",
        "\n",
        "# ============================\n",
        "#        MONITOR.PY\n",
        "# ============================\n",
        "\n",
        "def interact(env, agent, num_episodes=20000, window=100,\n",
        "             show_progress=False, endline=\"\"):\n",
        "\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes+1):\n",
        "\n",
        "        state, info = env.reset()  # Gymnasium returns (obs, info)\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= 100:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        if show_progress and not i_episode % show_progress:\n",
        "            print(\n",
        "                f\"\\rEpisode {i_episode}/{num_episodes} || epsilon={agent.epsilon:.7f}, Best avg reward {best_avg_reward}\",\n",
        "                end=endline\n",
        "            )\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        if best_avg_reward >= 9.7:\n",
        "            print(f\"\\nEnvironment solved in {i_episode} episodes.\")\n",
        "            break\n",
        "\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ============================\n",
        "#         MAIN.PY\n",
        "# ============================\n",
        "\n",
        "n_episodes = 150000\n",
        "nruns = 1\n",
        "medsub = nruns // 2\n",
        "\n",
        "beta = .7\n",
        "c1 = .02\n",
        "c2 = 3\n",
        "alpha = .7\n",
        "gamma = .5\n",
        "a = -.005\n",
        "b = 5e-5\n",
        "eps_min = 0\n",
        "epfunc = lambda i: max(eps_min, math.exp(a - b*i))\n",
        "\n",
        "best_avg_rewards = []\n",
        "local_seed = 80\n",
        "start = 3\n",
        "\n",
        "print(\"\\n\\nBegin Taxi-v3 learning...\")\n",
        "\n",
        "for i in range(start, start + nruns):\n",
        "\n",
        "    env = gym.make(\"Taxi-v3\")\n",
        "\n",
        "    # Proper Gymnasium seeding\n",
        "    env.reset(seed=10000*i + local_seed)\n",
        "    random.seed(i + local_seed)\n",
        "    np.random.seed(100*i + local_seed)\n",
        "\n",
        "    agent = Agent(alpha=alpha, gamma=gamma, get_epsilon=epfunc,\n",
        "                  c1=c1, c2=c2, beta=beta)\n",
        "\n",
        "    avg_rewards, best_avg_reward = interact(\n",
        "        env, agent, n_episodes, show_progress=10000, endline=\"\\n\"\n",
        "    )\n",
        "\n",
        "    best_avg_rewards.append(best_avg_reward)\n",
        "    print(f\"Run {i}/{nruns}, avg so far = {sum(best_avg_rewards)/len(best_avg_rewards)}\")\n",
        "\n",
        "\n",
        "print(\"\\nLocal seed:\", local_seed)\n",
        "print(\"Average:\", sum(best_avg_rewards)/len(best_avg_rewards))\n",
        "print(\"Median:\", sorted(best_avg_rewards)[medsub])\n",
        "print(np.array(sorted(best_avg_rewards)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l5uS1WAhGXLf",
        "outputId": "44f49858-7054-4351-d772-ef7172374241"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-3.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting colorama>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from bayesian-optimization) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bayesian-optimization) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bayesian-optimization) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->bayesian-optimization) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.0.0->bayesian-optimization) (3.6.0)\n",
            "Downloading bayesian_optimization-3.1.0-py3-none-any.whl (36 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-3.1.0 colorama-0.4.6\n"
          ]
        }
      ],
      "source": [
        "!pip install bayesian-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjuktPUYEPU6",
        "outputId": "fd8d1163-ebdd-4bf6-b72f-94b0decf509f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Bayesian Optimization...\n",
            "============================================================\n",
            "|   iter    |  target   |    eps    | eps_decay |   alpha   |   gamma   |\n",
            "-------------------------------------------------------------------------\n",
            "Tested: eps=0.850, eps_decay=0.600, alpha=0.150, gamma=0.900 -> 8.91\n",
            "| \u001b[39m1        \u001b[39m | \u001b[39m8.91     \u001b[39m | \u001b[39m0.85     \u001b[39m | \u001b[39m0.6      \u001b[39m | \u001b[39m0.15     \u001b[39m | \u001b[39m0.9      \u001b[39m |\n",
            "Tested: eps=1.000, eps_decay=0.450, alpha=0.050, gamma=0.850 -> 8.74\n",
            "| \u001b[39m2        \u001b[39m | \u001b[39m8.74     \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.45     \u001b[39m | \u001b[39m0.05     \u001b[39m | \u001b[39m0.85     \u001b[39m |\n",
            "Tested: eps=0.900, eps_decay=0.500, alpha=0.100, gamma=0.900 -> 8.77\n",
            "| \u001b[39m3        \u001b[39m | \u001b[39m8.77     \u001b[39m | \u001b[39m0.9      \u001b[39m | \u001b[39m0.5      \u001b[39m | \u001b[39m0.1      \u001b[39m | \u001b[39m0.9      \u001b[39m |\n",
            "Tested: eps=0.687, eps_decay=0.923, alpha=0.233, gamma=0.914 -> 8.86\n",
            "| \u001b[39m4        \u001b[39m | \u001b[39m8.86     \u001b[39m | \u001b[39m0.6872700\u001b[39m | \u001b[39m0.9228928\u001b[39m | \u001b[39m0.2329984\u001b[39m | \u001b[39m0.9137451\u001b[39m |\n",
            "Tested: eps=0.578, eps_decay=0.486, alpha=0.065, gamma=0.965 -> 8.67\n",
            "| \u001b[39m5        \u001b[39m | \u001b[39m8.67     \u001b[39m | \u001b[39m0.5780093\u001b[39m | \u001b[39m0.4857969\u001b[39m | \u001b[39m0.0645209\u001b[39m | \u001b[39m0.9645734\u001b[39m |\n",
            "Tested: eps=0.801, eps_decay=0.789, alpha=0.055, gamma=0.984 -> 8.88\n",
            "| \u001b[39m6        \u001b[39m | \u001b[39m8.88     \u001b[39m | \u001b[39m0.8005575\u001b[39m | \u001b[39m0.7894399\u001b[39m | \u001b[39m0.0551461\u001b[39m | \u001b[39m0.9842828\u001b[39m |\n",
            "Tested: eps=0.916, eps_decay=0.517, alpha=0.095, gamma=0.835 -> 8.72\n",
            "| \u001b[39m7        \u001b[39m | \u001b[39m8.72     \u001b[39m | \u001b[39m0.9162213\u001b[39m | \u001b[39m0.5167865\u001b[39m | \u001b[39m0.0954562\u001b[39m | \u001b[39m0.8348468\u001b[39m |\n",
            "Tested: eps=0.652, eps_decay=0.689, alpha=0.158, gamma=0.855 -> 8.73\n",
            "| \u001b[39m8        \u001b[39m | \u001b[39m8.73     \u001b[39m | \u001b[39m0.6521211\u001b[39m | \u001b[39m0.6886160\u001b[39m | \u001b[39m0.1579862\u001b[39m | \u001b[39m0.8553335\u001b[39m |\n",
            "Tested: eps=0.843, eps_decay=0.675, alpha=0.122, gamma=0.945 -> 8.69\n",
            "| \u001b[39m9        \u001b[39m | \u001b[39m8.69     \u001b[39m | \u001b[39m0.8431476\u001b[39m | \u001b[39m0.6745052\u001b[39m | \u001b[39m0.1218619\u001b[39m | \u001b[39m0.9447369\u001b[39m |\n",
            "Tested: eps=0.849, eps_decay=0.602, alpha=0.149, gamma=0.903 -> 8.64\n",
            "| \u001b[39m10       \u001b[39m | \u001b[39m8.64     \u001b[39m | \u001b[39m0.8486066\u001b[39m | \u001b[39m0.6015535\u001b[39m | \u001b[39m0.1493320\u001b[39m | \u001b[39m0.9033248\u001b[39m |\n",
            "Tested: eps=0.730, eps_decay=0.511, alpha=0.073, gamma=0.860 -> 8.81\n",
            "| \u001b[39m11       \u001b[39m | \u001b[39m8.81     \u001b[39m | \u001b[39m0.7297673\u001b[39m | \u001b[39m0.5113000\u001b[39m | \u001b[39m0.0726330\u001b[39m | \u001b[39m0.8598574\u001b[39m |\n",
            "Tested: eps=0.760, eps_decay=0.467, alpha=0.070, gamma=0.819 -> 8.83\n",
            "| \u001b[39m12       \u001b[39m | \u001b[39m8.83     \u001b[39m | \u001b[39m0.7595031\u001b[39m | \u001b[39m0.4665636\u001b[39m | \u001b[39m0.0695994\u001b[39m | \u001b[39m0.8191448\u001b[39m |\n",
            "Tested: eps=0.763, eps_decay=0.488, alpha=0.257, gamma=0.941 -> 9.00\n",
            "| \u001b[35m13       \u001b[39m | \u001b[35m9.0      \u001b[39m | \u001b[35m0.7626461\u001b[39m | \u001b[35m0.4877481\u001b[39m | \u001b[35m0.2565232\u001b[39m | \u001b[35m0.9412518\u001b[39m |\n",
            "Tested: eps=0.975, eps_decay=0.939, alpha=0.202, gamma=0.895 -> 8.75\n",
            "| \u001b[39m14       \u001b[39m | \u001b[39m8.75     \u001b[39m | \u001b[39m0.9745091\u001b[39m | \u001b[39m0.9391437\u001b[39m | \u001b[39m0.2022390\u001b[39m | \u001b[39m0.8946267\u001b[39m |\n",
            "Tested: eps=0.627, eps_decay=0.931, alpha=0.072, gamma=0.928 -> 8.79\n",
            "| \u001b[39m15       \u001b[39m | \u001b[39m8.79     \u001b[39m | \u001b[39m0.6274374\u001b[39m | \u001b[39m0.9313397\u001b[39m | \u001b[39m0.0716777\u001b[39m | \u001b[39m0.9284880\u001b[39m |\n",
            "Tested: eps=0.547, eps_decay=0.934, alpha=0.282, gamma=0.975 -> 8.67\n",
            "| \u001b[39m16       \u001b[39m | \u001b[39m8.67     \u001b[39m | \u001b[39m0.5468397\u001b[39m | \u001b[39m0.9337098\u001b[39m | \u001b[39m0.2819438\u001b[39m | \u001b[39m0.9747169\u001b[39m |\n",
            "Tested: eps=0.772, eps_decay=0.949, alpha=0.162, gamma=0.903 -> 8.70\n",
            "| \u001b[39m17       \u001b[39m | \u001b[39m8.7      \u001b[39m | \u001b[39m0.7716919\u001b[39m | \u001b[39m0.9488018\u001b[39m | \u001b[39m0.1624221\u001b[39m | \u001b[39m0.9029592\u001b[39m |\n",
            "Tested: eps=0.718, eps_decay=0.684, alpha=0.101, gamma=0.948 -> 8.92\n",
            "| \u001b[39m18       \u001b[39m | \u001b[39m8.92     \u001b[39m | \u001b[39m0.7179555\u001b[39m | \u001b[39m0.6842690\u001b[39m | \u001b[39m0.1013302\u001b[39m | \u001b[39m0.9481382\u001b[39m |\n",
            "Tested: eps=0.799, eps_decay=0.682, alpha=0.234, gamma=0.885 -> 8.88\n",
            "| \u001b[39m19       \u001b[39m | \u001b[39m8.88     \u001b[39m | \u001b[39m0.7989410\u001b[39m | \u001b[39m0.6818358\u001b[39m | \u001b[39m0.2341381\u001b[39m | \u001b[39m0.8854058\u001b[39m |\n",
            "Tested: eps=0.768, eps_decay=0.735, alpha=0.144, gamma=0.931 -> 8.77\n",
            "| \u001b[39m20       \u001b[39m | \u001b[39m8.77     \u001b[39m | \u001b[39m0.7679775\u001b[39m | \u001b[39m0.7349793\u001b[39m | \u001b[39m0.1438303\u001b[39m | \u001b[39m0.9307928\u001b[39m |\n",
            "Tested: eps=0.860, eps_decay=0.592, alpha=0.061, gamma=0.867 -> 8.85\n",
            "| \u001b[39m21       \u001b[39m | \u001b[39m8.85     \u001b[39m | \u001b[39m0.8597744\u001b[39m | \u001b[39m0.5916588\u001b[39m | \u001b[39m0.0611785\u001b[39m | \u001b[39m0.8670712\u001b[39m |\n",
            "Tested: eps=0.626, eps_decay=0.710, alpha=0.238, gamma=0.931 -> 8.96\n",
            "| \u001b[39m22       \u001b[39m | \u001b[39m8.96     \u001b[39m | \u001b[39m0.6263395\u001b[39m | \u001b[39m0.7096510\u001b[39m | \u001b[39m0.2377882\u001b[39m | \u001b[39m0.9309104\u001b[39m |\n",
            "Tested: eps=0.650, eps_decay=0.482, alpha=0.061, gamma=0.941 -> 8.73\n",
            "| \u001b[39m23       \u001b[39m | \u001b[39m8.73     \u001b[39m | \u001b[39m0.6504754\u001b[39m | \u001b[39m0.4815027\u001b[39m | \u001b[39m0.0612949\u001b[39m | \u001b[39m0.9413810\u001b[39m |\n",
            "Tested: eps=0.780, eps_decay=0.461, alpha=0.175, gamma=0.831 -> 8.60\n",
            "| \u001b[39m24       \u001b[39m | \u001b[39m8.6      \u001b[39m | \u001b[39m0.7800410\u001b[39m | \u001b[39m0.4605023\u001b[39m | \u001b[39m0.1752590\u001b[39m | \u001b[39m0.8308744\u001b[39m |\n",
            "Tested: eps=0.915, eps_decay=0.890, alpha=0.167, gamma=0.879 -> 8.74\n",
            "| \u001b[39m25       \u001b[39m | \u001b[39m8.74     \u001b[39m | \u001b[39m0.9148488\u001b[39m | \u001b[39m0.8895682\u001b[39m | \u001b[39m0.1674644\u001b[39m | \u001b[39m0.8787584\u001b[39m |\n",
            "Tested: eps=0.578, eps_decay=0.748, alpha=0.207, gamma=0.947 -> 8.78\n",
            "| \u001b[39m26       \u001b[39m | \u001b[39m8.78     \u001b[39m | \u001b[39m0.5781301\u001b[39m | \u001b[39m0.7475065\u001b[39m | \u001b[39m0.2073320\u001b[39m | \u001b[39m0.9470342\u001b[39m |\n",
            "Tested: eps=0.833, eps_decay=0.929, alpha=0.192, gamma=0.978 -> 9.14\n",
            "| \u001b[35m27       \u001b[39m | \u001b[35m9.14     \u001b[39m | \u001b[35m0.8329292\u001b[39m | \u001b[35m0.9292542\u001b[39m | \u001b[35m0.1918204\u001b[39m | \u001b[35m0.9781224\u001b[39m |\n",
            "Tested: eps=0.978, eps_decay=0.742, alpha=0.175, gamma=0.828 -> 8.84\n",
            "| \u001b[39m28       \u001b[39m | \u001b[39m8.84     \u001b[39m | \u001b[39m0.9775885\u001b[39m | \u001b[39m0.7423943\u001b[39m | \u001b[39m0.1752678\u001b[39m | \u001b[39m0.8278549\u001b[39m |\n",
            "=========================================================================\n",
            "\n",
            "============================================================\n",
            "OPTIMIZATION COMPLETE\n",
            "============================================================\n",
            "\n",
            "Best parameters found:\n",
            "  eps:       0.8329\n",
            "  eps_decay: 0.9293\n",
            "  alpha:     0.1918\n",
            "  gamma:     0.9781\n",
            "\n",
            "Best reward: 9.14\n",
            "\n",
            "============================================================\n",
            "TESTING BEST CONFIGURATION (5 runs)\n",
            "============================================================\n",
            "Run 1: 8.79\n",
            "Run 2: 8.73\n",
            "Run 3: 8.63\n",
            "Run 4: 8.92\n",
            "Run 5: 9.05\n",
            "\n",
            "Average over 5 runs: 8.82 ± 0.15\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "\n",
        "# ============================\n",
        "#         AGENT\n",
        "# ============================\n",
        "class Agent:\n",
        "    def __init__(self, nA=6, eps=0.1, eps_decay=0.9, alpha=0.3, gamma=0.9, mode='expected'):\n",
        "        \"\"\"Initialize agent with configurable hyperparameters.\"\"\"\n",
        "        self.nA = nA\n",
        "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
        "        self.num_episodes = 0\n",
        "\n",
        "        self.eps = eps\n",
        "        self.min_eps = 0.0\n",
        "        self.eps_decay = eps_decay\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.mode = mode  # 'q-learning' or 'expected'\n",
        "\n",
        "    def _get_probs(self, state):\n",
        "        \"\"\"Get action probabilities for epsilon-greedy policy.\"\"\"\n",
        "        max_i = np.argmax(self.Q[state])\n",
        "        return [\n",
        "            (1 - self.eps + self.eps / self.nA) if i == max_i else self.eps / self.nA\n",
        "            for i in range(self.nA)\n",
        "        ]\n",
        "\n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
        "        return np.random.choice(range(self.nA), p=self._get_probs(state))\n",
        "\n",
        "    def _update_params(self):\n",
        "        \"\"\"Decay epsilon after each episode.\"\"\"\n",
        "        self.eps = max(self.min_eps, self.eps * self.eps_decay)\n",
        "\n",
        "    def step(self, s, a, r, s_next, done):\n",
        "        \"\"\"Update Q-values using Q-learning or Expected SARSA.\"\"\"\n",
        "        if self.mode == 'q-learning':\n",
        "            a_best = np.argmax(self.Q[s_next])\n",
        "            exp_val = self.Q[s_next][a_best]\n",
        "        elif self.mode == 'expected':\n",
        "            # Expected SARSA: weighted average over all actions\n",
        "            exp_val = sum(x * y for x, y in zip(self.Q[s_next], self._get_probs(s_next)))\n",
        "        else:\n",
        "            raise RuntimeError('Unsupported mode')\n",
        "\n",
        "        # TD update\n",
        "        self.Q[s][a] = (1 - self.alpha) * self.Q[s][a] + self.alpha * (r + self.gamma * exp_val)\n",
        "\n",
        "        if done:\n",
        "            self.num_episodes += 1\n",
        "            self._update_params()\n",
        "\n",
        "\n",
        "# ============================\n",
        "#       MONITOR\n",
        "# ============================\n",
        "def interact(env, agent, num_episodes=20000, window=100, verbose=False):\n",
        "    \"\"\"Monitor agent's performance.\"\"\"\n",
        "    avg_rewards = deque(maxlen=num_episodes)\n",
        "    best_avg_reward = -math.inf\n",
        "    samp_rewards = deque(maxlen=window)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        state, info = env.reset()\n",
        "        samp_reward = 0\n",
        "\n",
        "        while True:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            samp_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                samp_rewards.append(samp_reward)\n",
        "                break\n",
        "\n",
        "        if i_episode >= window:\n",
        "            avg_reward = np.mean(samp_rewards)\n",
        "            avg_rewards.append(avg_reward)\n",
        "            if avg_reward > best_avg_reward:\n",
        "                best_avg_reward = avg_reward\n",
        "\n",
        "        if verbose and i_episode % 1000 == 0:\n",
        "            print(f\"\\rEpisode {i_episode}/{num_episodes} || Best avg {best_avg_reward:.2f}\", end=\"\")\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        if best_avg_reward >= 9.7:\n",
        "            if verbose:\n",
        "                print(f'\\nEnvironment solved in {i_episode} episodes.')\n",
        "            break\n",
        "\n",
        "    if verbose:\n",
        "        print()\n",
        "    return avg_rewards, best_avg_reward\n",
        "\n",
        "\n",
        "# ============================\n",
        "#       BAYESIAN OPTIMIZATION\n",
        "# ============================\n",
        "def interact_wrapper(eps, eps_decay, alpha, gamma):\n",
        "    \"\"\"Wrapper function for Bayesian optimization.\"\"\"\n",
        "    env = gym.make('Taxi-v3')\n",
        "    agent = Agent(\n",
        "        nA=env.action_space.n,\n",
        "        eps=eps,\n",
        "        eps_decay=eps_decay,\n",
        "        alpha=alpha,\n",
        "        gamma=gamma,\n",
        "        mode='expected'  # Use Expected SARSA\n",
        "    )\n",
        "    _, best_avg_reward = interact(env, agent, num_episodes=20000, window=100, verbose=False)\n",
        "    print(f\"Tested: eps={eps:.3f}, eps_decay={eps_decay:.3f}, alpha={alpha:.3f}, gamma={gamma:.3f} -> {best_avg_reward:.2f}\")\n",
        "    return best_avg_reward\n",
        "\n",
        "\n",
        "# ============================\n",
        "#        MAIN\n",
        "# ============================\n",
        "if __name__ == '__main__':\n",
        "    # Define parameter bounds for Bayesian optimization\n",
        "    pbounds = {\n",
        "        'eps': (0.5, 1.0),           # Initial epsilon\n",
        "        'eps_decay': (0.4, 0.95),    # Epsilon decay rate\n",
        "        'alpha': (0.05, 0.3),        # Learning rate\n",
        "        'gamma': (0.8, 0.99)         # Discount factor\n",
        "    }\n",
        "\n",
        "    # Initialize Bayesian optimizer\n",
        "    optimizer = BayesianOptimization(\n",
        "        f=interact_wrapper,\n",
        "        pbounds=pbounds,\n",
        "        random_state=42,\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    # Add some good starting points based on research\n",
        "    optimizer.probe(\n",
        "        params={'eps': 0.85, 'eps_decay': 0.6, 'alpha': 0.15, 'gamma': 0.9},\n",
        "        lazy=True,\n",
        "    )\n",
        "    optimizer.probe(\n",
        "        params={'eps': 1.0, 'eps_decay': 0.45, 'alpha': 0.05, 'gamma': 0.85},\n",
        "        lazy=True,\n",
        "    )\n",
        "    optimizer.probe(\n",
        "        params={'eps': 0.9, 'eps_decay': 0.5, 'alpha': 0.1, 'gamma': 0.9},\n",
        "        lazy=True,\n",
        "    )\n",
        "\n",
        "    print(\"Starting Bayesian Optimization...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Run optimization\n",
        "    optimizer.maximize(\n",
        "        init_points=5,    # Random exploration points\n",
        "        n_iter=20         # Optimization iterations\n",
        "    )\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"OPTIMIZATION COMPLETE\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nBest parameters found:\")\n",
        "    print(f\"  eps:       {optimizer.max['params']['eps']:.4f}\")\n",
        "    print(f\"  eps_decay: {optimizer.max['params']['eps_decay']:.4f}\")\n",
        "    print(f\"  alpha:     {optimizer.max['params']['alpha']:.4f}\")\n",
        "    print(f\"  gamma:     {optimizer.max['params']['gamma']:.4f}\")\n",
        "    print(f\"\\nBest reward: {optimizer.max['target']:.2f}\")\n",
        "\n",
        "    # Test the best configuration multiple times\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"TESTING BEST CONFIGURATION (5 runs)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    best_params = optimizer.max['params']\n",
        "    test_rewards = []\n",
        "\n",
        "    for run in range(5):\n",
        "        env = gym.make('Taxi-v3')\n",
        "        agent = Agent(\n",
        "            nA=env.action_space.n,\n",
        "            eps=best_params['eps'],\n",
        "            eps_decay=best_params['eps_decay'],\n",
        "            alpha=best_params['alpha'],\n",
        "            gamma=best_params['gamma'],\n",
        "            mode='expected'\n",
        "        )\n",
        "        _, reward = interact(env, agent, num_episodes=20000, window=100, verbose=False)\n",
        "        test_rewards.append(reward)\n",
        "        print(f\"Run {run+1}: {reward:.2f}\")\n",
        "\n",
        "    print(f\"\\nAverage over 5 runs: {np.mean(test_rewards):.2f} ± {np.std(test_rewards):.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}